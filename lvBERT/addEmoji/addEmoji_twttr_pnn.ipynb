{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2616b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a84c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da5ece",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47471ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../../labeledTweets/allLabeledTweets.csv')\n",
    "df = df[['id', 'message', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newLine =\"\\\\n|\\\\r\"\n",
    "urls = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "numbers = '\\d+((\\.|\\-)\\d+)?'\n",
    "mentions = '\\B\\@([\\w\\-]+)'\n",
    "hashtag = '#'\n",
    "whitespaces = '\\s+'\n",
    "leadTrailWhitespace = '^\\s+|\\s+?$'\n",
    "\n",
    "df['clean_message'] = df['message']\n",
    "df['clean_message'] = df['clean_message'].str.replace(newLine,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(urls,' URL ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(mentions,' MENTION ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(numbers,' NMBR ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(hashtag,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(whitespaces,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(leadTrailWhitespace,'',regex=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc0d24",
   "metadata": {},
   "source": [
    "# Train, validate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051783f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6440d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63617ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./tweets_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb2b33",
   "metadata": {},
   "source": [
    "## Balance training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d625ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.data_type=='train']['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13204ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = [df[df.data_type=='train'].clean_message, df[df.data_type=='train'].label]\n",
    "df_train = pd.concat(df_train, axis=1, keys=[\"clean_message\", \"label\"])\n",
    "\n",
    "df_0 = df_train[df_train['label']==0]\n",
    "df_1 = df_train[df_train['label']==1]\n",
    "df_2 = df_train[df_train['label']==2]\n",
    "\n",
    "df_0_downsampled = df_0.sample(df_1.shape[0], random_state=42)\n",
    "df_2_downsampled = df_2.sample(df_1.shape[0], random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_0_downsampled, df_2_downsampled, df_1])\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ffe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle rows\n",
    "import sklearn\n",
    "\n",
    "df_train = sklearn.utils.shuffle(df_train, random_state=0)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843c4ae",
   "metadata": {},
   "source": [
    "# Tokenizer \"lvBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b43559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./../lvbert_pytorch/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bffc0",
   "metadata": {},
   "source": [
    "## Add emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoji_str = \"ğŸ˜€ ğŸ˜ƒ ğŸ˜„ ğŸ˜ ğŸ˜† ğŸ˜… ğŸ˜‚ ğŸ¤£ ğŸ˜Š ğŸ˜‡ ğŸ™‚ ğŸ™ƒ ğŸ˜‰ ğŸ˜Œ ğŸ˜ ğŸ¥° ğŸ˜˜ ğŸ˜— ğŸ˜™ ğŸ˜š ğŸ˜‹ ğŸ˜› ğŸ˜ ğŸ˜œ ğŸ¤ª ğŸ¤¨ ğŸ§ ğŸ¤“ ğŸ˜ ğŸ¤© ğŸ¥³ ğŸ˜ ğŸ˜’ ğŸ˜ ğŸ˜” ğŸ˜Ÿ ğŸ˜• ğŸ™ â˜¹ï¸ ğŸ˜£ ğŸ˜– ğŸ˜« ğŸ˜© ğŸ¥º ğŸ˜¢ ğŸ˜­ ğŸ˜¤ ğŸ˜  ğŸ˜¡ ğŸ¤¬ ğŸ¤¯ ğŸ˜³ ğŸ¥µ ğŸ¥¶ ğŸ˜± ğŸ˜¨ ğŸ˜° ğŸ˜¥ ğŸ˜“ ğŸ¤— ğŸ¤” ğŸ¤­ ğŸ¤« ğŸ¤¥ ğŸ˜¶ ğŸ˜ ğŸ˜‘ ğŸ˜¬ ğŸ™„ ğŸ˜¯ ğŸ˜¦ ğŸ˜§ ğŸ˜® ğŸ˜² ğŸ¥± ğŸ˜´ ğŸ¤¤ ğŸ˜ª ğŸ˜µ ğŸ¤ ğŸ¥´ ğŸ¤¢ ğŸ¤® ğŸ¤§ ğŸ˜· ğŸ¤’ ğŸ¤• ğŸ˜º ğŸ˜¸ ğŸ˜¹ ğŸ˜» ğŸ˜¼ ğŸ˜½ ğŸ™€ ğŸ˜¿ ğŸ˜¾ ğŸ’© ğŸ‘‹ ğŸ¤š ğŸ– âœ‹ ğŸ–– ğŸ‘Œ ğŸ¤ âœŒï¸ ğŸ¤ ğŸ¤Ÿ ğŸ¤˜ ğŸ¤™ ğŸ‘ˆ ğŸ‘‰ ğŸ‘† ğŸ–• ğŸ‘‡ â˜ï¸ ğŸ‘ ğŸ‘ âœŠ ğŸ‘Š ğŸ¤› ğŸ¤œ ğŸ‘ ğŸ™Œ ğŸ‘ ğŸ¤² ğŸ¤ ğŸ™'ğŸ‘‹ğŸ» ğŸ¤šğŸ» ğŸ–ğŸ» âœ‹ğŸ» ğŸ––ğŸ» ğŸ‘ŒğŸ» ğŸ¤ğŸ» âœŒğŸ» ğŸ¤ğŸ» ğŸ¤ŸğŸ» ğŸ¤˜ğŸ» ğŸ¤™ğŸ» ğŸ‘ˆğŸ» ğŸ‘‰ğŸ» ğŸ‘†ğŸ» ğŸ–•ğŸ» ğŸ‘‡ğŸ» â˜ğŸ» ğŸ‘ğŸ» ğŸ‘ğŸ» âœŠğŸ» ğŸ‘ŠğŸ» ğŸ¤›ğŸ» ğŸ¤œğŸ» ğŸ‘ğŸ» ğŸ™ŒğŸ» ğŸ‘ğŸ» ğŸ¤²ğŸ» ğŸ™ğŸ» ğŸ‘‹ğŸ¼ ğŸ¤šğŸ¼ ğŸ–ğŸ¼ âœ‹ğŸ¼ ğŸ––ğŸ¼ ğŸ‘ŒğŸ¼ ğŸ¤ğŸ¼ âœŒğŸ¼ ğŸ¤ğŸ¼ ğŸ¤ŸğŸ¼ ğŸ¤˜ğŸ¼ ğŸ¤™ğŸ¼ ğŸ‘ˆğŸ¼ ğŸ‘‰ğŸ¼ ğŸ‘†ğŸ¼ ğŸ–•ğŸ¼ ğŸ‘‡ğŸ¼ â˜ğŸ¼ ğŸ‘ğŸ¼ ğŸ‘ğŸ¼ âœŠğŸ¼ ğŸ‘ŠğŸ¼ ğŸ¤›ğŸ¼ ğŸ¤œğŸ¼ ğŸ‘ğŸ¼ ğŸ™ŒğŸ¼ ğŸ‘ğŸ¼ ğŸ¤²ğŸ¼ ğŸ™ğŸ¼ ğŸ‘‹ğŸ½ ğŸ¤šğŸ½ ğŸ–ğŸ½ âœ‹ğŸ½ ğŸ––ğŸ½ ğŸ‘ŒğŸ½ ğŸ¤ğŸ½ âœŒğŸ½ ğŸ¤ğŸ½ ğŸ¤ŸğŸ½ ğŸ¤˜ğŸ½ ğŸ¤™ğŸ½ ğŸ‘ˆğŸ½ ğŸ‘‰ğŸ½ ğŸ‘†ğŸ½ ğŸ–•ğŸ½ ğŸ‘‡ğŸ½ â˜ğŸ½ ğŸ‘ğŸ½ ğŸ‘ğŸ½ âœŠğŸ½ ğŸ‘ŠğŸ½ ğŸ¤›ğŸ½ ğŸ¤œğŸ½ ğŸ‘ğŸ½ ğŸ™ŒğŸ½ ğŸ‘ğŸ½ ğŸ¤²ğŸ½ ğŸ™ğŸ‘‹ğŸ¾ ğŸ¤šğŸ¾ ğŸ–ğŸ¾ âœ‹ğŸ¾ ğŸ––ğŸ¾ ğŸ‘ŒğŸ¾ ğŸ¤ğŸ¾ âœŒğŸ¾ ğŸ¤ğŸ¾ ğŸ¤ŸğŸ¾ ğŸ¤˜ğŸ¾ ğŸ¤™ğŸ¾ ğŸ‘ˆğŸ¾ ğŸ‘‰ğŸ¾ ğŸ‘†ğŸ¾ ğŸ–•ğŸ¾ ğŸ‘‡ğŸ¾ â˜ğŸ¾ ğŸ‘ğŸ¾ ğŸ‘ğŸ¾ âœŠğŸ¾ ğŸ‘ŠğŸ¾ ğŸ¤›ğŸ¾ ğŸ¤œğŸ¾ ğŸ‘ğŸ¾ ğŸ™ŒğŸ¾ ğŸ‘ğŸ¾ ğŸ¤²ğŸ¾ ğŸ™ ğŸ‘‹ğŸ¿ ğŸ¤šğŸ¿ ğŸ–ğŸ¿ âœ‹ğŸ¿ ğŸ––ğŸ¿ ğŸ‘ŒğŸ¿ ğŸ¤ğŸ¿ âœŒğŸ¿ ğŸ¤ğŸ¿ ğŸ¤ŸğŸ¿ ğŸ¤˜ğŸ¿ ğŸ¤™ğŸ¿ ğŸ‘ˆğŸ¿ ğŸ‘‰ğŸ¿ ğŸ‘†ğŸ¿ ğŸ–•ğŸ¿ ğŸ‘‡ğŸ¿ â˜ğŸ¿ ğŸ‘ğŸ¿ ğŸ‘ğŸ¿ âœŠğŸ¿ ğŸ‘ŠğŸ¿ ğŸ¤›ğŸ¿ ğŸ¤œğŸ¿ ğŸ‘ğŸ¿ ğŸ™ŒğŸ¿ ğŸ‘ğŸ¿ ğŸ¤²ğŸ¿ ğŸ™ğŸ¿ â¤ï¸ ğŸ§¡ ğŸ’› ğŸ’š ğŸ’™ ğŸ’œ ğŸ–¤ ğŸ¤ ğŸ¤ ğŸ’” â£ï¸ ğŸ’• ğŸ’ ğŸ’“ ğŸ’— ğŸ’– ğŸ’˜ ğŸ’ ğŸ’Ÿ ğŸ’‘ğŸ» ğŸ’‘ğŸ¼ ğŸ’‘ğŸ½ ğŸ’‘ğŸ¾ ğŸ’‘ğŸ¿ ğŸ’ğŸ» ğŸ’ğŸ¼ ğŸ’ğŸ½ ğŸ’ğŸ¾ ğŸ’ğŸ¿ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ‘­ ğŸ§‘â€ğŸ¤â€ğŸ§‘ ğŸ‘¬ ğŸ‘« ğŸ‘©â€â¤ï¸â€ğŸ‘© ğŸ’‘ ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘© ğŸ’ ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ ğŸ‘ª ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ ğŸ‘§â€ğŸ‘§ ğŸ’‹\"\n",
    "emoji_list = emoji_str.split(' ')\n",
    "emoji_regex = '|'.join(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emoji = []\n",
    "for message in df.clean_message:\n",
    "    foundEmoji = re.findall(emoji_regex, message)\n",
    "    for emoji in foundEmoji:\n",
    "        all_emoji.append(emoji)\n",
    "\n",
    "print(len(all_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter=collections.Counter(all_emoji)\n",
    "print(counter.most_common(100))\n",
    "\n",
    "most_common_values= [word for word, word_count in counter.most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(most_common_values, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other UNK tokens\n",
    "\n",
    "unk_tokens = []\n",
    "for message in df.clean_message.values:\n",
    "    list_of_space_separated_pieces = message.strip().split()\n",
    "    ids = [tokenizer(piece, add_special_tokens=False)[\"input_ids\"] for piece in list_of_space_separated_pieces]\n",
    "    unk_indices = [i for i, encoded in enumerate(ids) if tokenizer.unk_token_id in encoded]\n",
    "    unknown_strings = [piece for i, piece in enumerate(list_of_space_separated_pieces) if i in unk_indices]\n",
    "    for unk_str in unknown_strings:\n",
    "        unk_tokens.append(unk_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter=collections.Counter(unk_tokens)\n",
    "print(counter.most_common(100))\n",
    "\n",
    "most_common_values= [word for word, word_count in counter.most_common(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504acea3",
   "metadata": {},
   "source": [
    "### Find max length for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b28671",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "for txt in list(df.clean_message.values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "    \n",
    "sns.displot(token_lens)\n",
    "plt.xlim([0, 125])\n",
    "plt.xlabel('Token count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e882ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6402cf",
   "metadata": {},
   "source": [
    "### Encode messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb945e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_train[\"clean_message\"].values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].clean_message.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset_train, './datasetsLowercase/dataset_train.pt')\n",
    "# torch.save(dataset_val, './datasetsLowercase/dataset_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = torch.load('./datasetsLowercase/dataset_train.pt')\n",
    "# dataset_val = torch.load('./datasetsLowercase/dataset_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11af821",
   "metadata": {},
   "source": [
    "# Model \"lvBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('./../lvbert_pytorch/',\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f447a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16189453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d95dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf40782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure weighted F1\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ccb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model. Returns average validation loss, predictions, true values\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_val, desc='Validating:', leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ef371",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'modelsEmoji/finetuned_lvBERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "    \n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    \n",
    "    print('Classification report:')\n",
    "    print(classification_report(true_vals, preds_flat))\n",
    "    print('Confusion matrix:')\n",
    "    print(pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "            index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "            columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']]))\n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271f1a9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b344990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelsEmoji/finetuned_BERT_epoch_X.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(true_vals, preds_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "        index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "        columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38twttr",
   "language": "python",
   "name": "venv38twttr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
