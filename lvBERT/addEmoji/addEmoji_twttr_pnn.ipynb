{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2616b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "66a84c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da5ece",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "47471ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic “potenciālie pircēji ir no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label\n",
       "0  1478404  Tiek vērtēti trīs potenciālie airBaltic invest...      0\n",
       "1  1478695  Augulis: #airBaltic “potenciālie pircēji ir no...      0\n",
       "2  1478812  airBaltic uzsāks lidojumus uz diviem jauniem g...      0\n",
       "3  1479295  Ministrs: Sarunas turpinās ar trīs potenciālaj...      0\n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../../labeledTweets/allLabeledTweets.csv')\n",
    "df = df[['id', 'message', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82da84e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    968\n",
       "2    647\n",
       "1    410\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b78b2f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic “potenciālie pircēji ir no...</td>\n",
       "      <td>0</td>\n",
       "      <td>Augulis: airBaltic “potenciālie pircēji ir no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "      <td>MENTION MENTION MENTION Netrāpīs kādam AirBalt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label  \\\n",
       "0  1478404  Tiek vērtēti trīs potenciālie airBaltic invest...      0   \n",
       "1  1478695  Augulis: #airBaltic “potenciālie pircēji ir no...      0   \n",
       "2  1478812  airBaltic uzsāks lidojumus uz diviem jauniem g...      0   \n",
       "3  1479295  Ministrs: Sarunas turpinās ar trīs potenciālaj...      0   \n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0   \n",
       "\n",
       "                                       clean_message  \n",
       "0  Tiek vērtēti trīs potenciālie airBaltic invest...  \n",
       "1  Augulis: airBaltic “potenciālie pircēji ir no ...  \n",
       "2  airBaltic uzsāks lidojumus uz diviem jauniem g...  \n",
       "3  Ministrs: Sarunas turpinās ar trīs potenciālaj...  \n",
       "4  MENTION MENTION MENTION Netrāpīs kādam AirBalt...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newLine =\"\\\\n|\\\\r\"\n",
    "urls = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "numbers = '\\d+((\\.|\\-)\\d+)?'\n",
    "mentions = '\\B\\@([\\w\\-]+)'\n",
    "hashtag = '#'\n",
    "whitespaces = '\\s+'\n",
    "leadTrailWhitespace = '^\\s+|\\s+?$'\n",
    "\n",
    "df['clean_message'] = df['message']\n",
    "df['clean_message'] = df['clean_message'].str.replace(newLine,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(urls,' URL ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(mentions,' MENTION ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(numbers,' NMBR ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(hashtag,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(whitespaces,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(leadTrailWhitespace,'',regex=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc0d24",
   "metadata": {},
   "source": [
    "# Train, validate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c1b2abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "051783f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  message  clean_message\n",
       "label data_type                             \n",
       "0     train      823      823            823\n",
       "      val        145      145            145\n",
       "1     train      348      348            348\n",
       "      val         62       62             62\n",
       "2     train      550      550            550\n",
       "      val         97       97             97"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6440d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic “potenciālie pircēji ir no...</td>\n",
       "      <td>0</td>\n",
       "      <td>Augulis: airBaltic “potenciālie pircēji ir no ...</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "      <td>MENTION MENTION MENTION Netrāpīs kādam AirBalt...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label  \\\n",
       "0  1478404  Tiek vērtēti trīs potenciālie airBaltic invest...      0   \n",
       "1  1478695  Augulis: #airBaltic “potenciālie pircēji ir no...      0   \n",
       "2  1478812  airBaltic uzsāks lidojumus uz diviem jauniem g...      0   \n",
       "3  1479295  Ministrs: Sarunas turpinās ar trīs potenciālaj...      0   \n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0   \n",
       "\n",
       "                                       clean_message data_type  \n",
       "0  Tiek vērtēti trīs potenciālie airBaltic invest...     train  \n",
       "1  Augulis: airBaltic “potenciālie pircēji ir no ...       val  \n",
       "2  airBaltic uzsāks lidojumus uz diviem jauniem g...     train  \n",
       "3  Ministrs: Sarunas turpinās ar trīs potenciālaj...     train  \n",
       "4  MENTION MENTION MENTION Netrāpīs kādam AirBalt...     train  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "63617ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./tweets_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb2b33",
   "metadata": {},
   "source": [
    "## Balance training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1d625ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    823\n",
       "2    550\n",
       "1    348\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.data_type=='train']['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13204ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    348\n",
       "1    348\n",
       "2    348\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = [df[df.data_type=='train'].clean_message, df[df.data_type=='train'].label]\n",
    "df_train = pd.concat(df_train, axis=1, keys=[\"clean_message\", \"label\"])\n",
    "\n",
    "df_0 = df_train[df_train['label']==0]\n",
    "df_1 = df_train[df_train['label']==1]\n",
    "df_2 = df_train[df_train['label']==2]\n",
    "\n",
    "df_0_downsampled = df_0.sample(df_1.shape[0], random_state=42)\n",
    "df_2_downsampled = df_2.sample(df_1.shape[0], random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_0_downsampled, df_2_downsampled, df_1])\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c86ffe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Šodien norisinās SEB MTB maratons. Finišs MENT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>MENTION Sveiks! Tas neattiecas uz mūsu ekspert...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>MENTION MENTION Starpcitu, Maxima tirgo labus ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>Taisnība ir uzvarējusi! Martins Dukurs kļūst p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>MENTION Спасибо за мнение.^el</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_message  label\n",
       "700   Šodien norisinās SEB MTB maratons. Finišs MENT...      0\n",
       "1911  MENTION Sveiks! Tas neattiecas uz mūsu ekspert...      1\n",
       "505   MENTION MENTION Starpcitu, Maxima tirgo labus ...      1\n",
       "1471  Taisnība ir uzvarējusi! Martins Dukurs kļūst p...      1\n",
       "1537                      MENTION Спасибо за мнение.^el      0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle rows\n",
    "import sklearn\n",
    "\n",
    "df_train = sklearn.utils.shuffle(df_train, random_state=0)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843c4ae",
   "metadata": {},
   "source": [
    "# Tokenizer \"lvBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "43b43559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./../lvbert_pytorch/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bffc0",
   "metadata": {},
   "source": [
    "## Add emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ade333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoji_str = \"😀 😃 😄 😁 😆 😅 😂 🤣 😊 😇 🙂 🙃 😉 😌 😍 🥰 😘 😗 😙 😚 😋 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏 😒 😞 😔 😟 😕 🙁 ☹️ 😣 😖 😫 😩 🥺 😢 😭 😤 😠 😡 🤬 🤯 😳 🥵 🥶 😱 😨 😰 😥 😓 🤗 🤔 🤭 🤫 🤥 😶 😐 😑 😬 🙄 😯 😦 😧 😮 😲 🥱 😴 🤤 😪 😵 🤐 🥴 🤢 🤮 🤧 😷 🤒 🤕 😺 😸 😹 😻 😼 😽 🙀 😿 😾 💩 👋 🤚 🖐 ✋ 🖖 👌 🤏 ✌️ 🤞 🤟 🤘 🤙 👈 👉 👆 🖕 👇 ☝️ 👍 👎 ✊ 👊 🤛 🤜 👏 🙌 👐 🤲 🤝 🙏'👋🏻 🤚🏻 🖐🏻 ✋🏻 🖖🏻 👌🏻 🤏🏻 ✌🏻 🤞🏻 🤟🏻 🤘🏻 🤙🏻 👈🏻 👉🏻 👆🏻 🖕🏻 👇🏻 ☝🏻 👍🏻 👎🏻 ✊🏻 👊🏻 🤛🏻 🤜🏻 👏🏻 🙌🏻 👐🏻 🤲🏻 🙏🏻 👋🏼 🤚🏼 🖐🏼 ✋🏼 🖖🏼 👌🏼 🤏🏼 ✌🏼 🤞🏼 🤟🏼 🤘🏼 🤙🏼 👈🏼 👉🏼 👆🏼 🖕🏼 👇🏼 ☝🏼 👍🏼 👎🏼 ✊🏼 👊🏼 🤛🏼 🤜🏼 👏🏼 🙌🏼 👐🏼 🤲🏼 🙏🏼 👋🏽 🤚🏽 🖐🏽 ✋🏽 🖖🏽 👌🏽 🤏🏽 ✌🏽 🤞🏽 🤟🏽 🤘🏽 🤙🏽 👈🏽 👉🏽 👆🏽 🖕🏽 👇🏽 ☝🏽 👍🏽 👎🏽 ✊🏽 👊🏽 🤛🏽 🤜🏽 👏🏽 🙌🏽 👐🏽 🤲🏽 🙏👋🏾 🤚🏾 🖐🏾 ✋🏾 🖖🏾 👌🏾 🤏🏾 ✌🏾 🤞🏾 🤟🏾 🤘🏾 🤙🏾 👈🏾 👉🏾 👆🏾 🖕🏾 👇🏾 ☝🏾 👍🏾 👎🏾 ✊🏾 👊🏾 🤛🏾 🤜🏾 👏🏾 🙌🏾 👐🏾 🤲🏾 🙏 👋🏿 🤚🏿 🖐🏿 ✋🏿 🖖🏿 👌🏿 🤏🏿 ✌🏿 🤞🏿 🤟🏿 🤘🏿 🤙🏿 👈🏿 👉🏿 👆🏿 🖕🏿 👇🏿 ☝🏿 👍🏿 👎🏿 ✊🏿 👊🏿 🤛🏿 🤜🏿 👏🏿 🙌🏿 👐🏿 🤲🏿 🙏🏿 ❤️ 🧡 💛 💚 💙 💜 🖤 🤍 🤎 💔 ❣️ 💕 💞 💓 💗 💖 💘 💝 💟 💑🏻 💑🏼 💑🏽 💑🏾 💑🏿 💏🏻 💏🏼 💏🏽 💏🏾 💏🏿 👨🏻‍❤️‍👨🏻 👨🏻‍❤️‍👨🏼 👨🏻‍❤️‍👨🏽 👨🏻‍❤️‍👨🏾 👨🏻‍❤️‍👨🏿 👨🏼‍❤️‍👨🏻 👨🏼‍❤️‍👨🏼 👨🏼‍❤️‍👨🏽 👨🏼‍❤️‍👨🏾 👨🏼‍❤️‍👨🏿 👨🏽‍❤️‍👨🏻 👨🏽‍❤️‍👨🏼 👨🏽‍❤️‍👨🏽 👨🏽‍❤️‍👨🏾 👨🏽‍❤️‍👨🏿 👨🏾‍❤️‍👨🏻 👨🏾‍❤️‍👨🏼 👨🏾‍❤️‍👨🏽 👨🏾‍❤️‍👨🏾 👨🏾‍❤️‍👨🏿 👨🏿‍❤️‍👨🏻 👨🏿‍❤️‍👨🏼 👨🏿‍❤️‍👨🏽 👨🏿‍❤️‍👨🏾 👨🏿‍❤️‍👨🏿 👩🏻‍❤️‍👨🏻 👩🏻‍❤️‍👨🏼 👩🏻‍❤️‍👨🏽 👩🏻‍❤️‍👨🏾 👩🏻‍❤️‍👨🏿 👩🏻‍❤️‍👩🏻 👩🏻‍❤️‍👩🏼 👩🏻‍❤️‍👩🏽 👩🏻‍❤️‍👩🏾 👩🏻‍❤️‍👩🏿 👩🏼‍❤️‍👨🏻 👩🏼‍❤️‍👨🏼 👩🏼‍❤️‍👨🏽 👩🏼‍❤️‍👨🏾 👩🏼‍❤️‍👨🏿 👩🏼‍❤️‍👩🏻 👩🏼‍❤️‍👩🏼 👩🏼‍❤️‍👩🏽 👩🏼‍❤️‍👩🏾 👩🏼‍❤️‍👩🏿 👩🏽‍❤️‍👨🏻 👩🏽‍❤️‍👨🏼 👩🏽‍❤️‍👨🏽 👩🏽‍❤️‍👨🏾 👩🏽‍❤️‍👨🏿 👩🏽‍❤️‍👩🏻 👩🏽‍❤️‍👩🏼 👩🏽‍❤️‍👩🏽 👩🏽‍❤️‍👩🏾 👩🏽‍❤️‍👩🏿 👩🏾‍❤️‍👨🏻 👩🏾‍❤️‍👨🏼 👩🏾‍❤️‍👨🏽 👩🏾‍❤️‍👨🏾 👩🏾‍❤️‍👨🏿 👩🏾‍❤️‍👩🏻 👩🏾‍❤️‍👩🏼 👩🏾‍❤️‍👩🏽 👩🏾‍❤️‍👩🏾 👩🏾‍❤️‍👩🏿 👩🏿‍❤️‍👨🏻 👩🏿‍❤️‍👨🏼 👩🏿‍❤️‍👨🏽 👩🏿‍❤️‍👨🏾 👩🏿‍❤️‍👨🏿 👩🏿‍❤️‍👩🏻 👩🏿‍❤️‍👩🏼 👩🏿‍❤️‍👩🏽 👩🏿‍❤️‍👩🏾 👩🏿‍❤️‍👩🏿 🧑🏻‍❤️‍🧑🏼 🧑🏻‍❤️‍🧑🏽 🧑🏻‍❤️‍🧑🏾 🧑🏻‍❤️‍🧑🏿 🧑🏼‍❤️‍🧑🏻 🧑🏼‍❤️‍🧑🏽 🧑🏼‍❤️‍🧑🏾 🧑🏼‍❤️‍🧑🏿 🧑🏽‍❤️‍🧑🏻 🧑🏽‍❤️‍🧑🏼 🧑🏽‍❤️‍🧑🏾 🧑🏽‍❤️‍🧑🏿 🧑🏾‍❤️‍🧑🏻 🧑🏾‍❤️‍🧑🏼 🧑🏾‍❤️‍🧑🏽 🧑🏾‍❤️‍🧑🏿 🧑🏿‍❤️‍🧑🏻 🧑🏿‍❤️‍🧑🏼 🧑🏿‍❤️‍🧑🏽 🧑🏿‍❤️‍🧑🏾 👨🏻‍❤️‍💋‍👨🏻 👨🏻‍❤️‍💋‍👨🏼 👨🏻‍❤️‍💋‍👨🏽 👨🏻‍❤️‍💋‍👨🏾 👨🏻‍❤️‍💋‍👨🏿 👨🏼‍❤️‍💋‍👨🏻 👨🏼‍❤️‍💋‍👨🏼 👨🏼‍❤️‍💋‍👨🏽 👨🏼‍❤️‍💋‍👨🏾 👨🏼‍❤️‍💋‍👨🏿 👨🏽‍❤️‍💋‍👨🏻 👨🏽‍❤️‍💋‍👨🏼 👨🏽‍❤️‍💋‍👨🏽 👨🏽‍❤️‍💋‍👨🏾 👨🏽‍❤️‍💋‍👨🏿 👨🏾‍❤️‍💋‍👨🏻 👨🏾‍❤️‍💋‍👨🏼 👨🏾‍❤️‍💋‍👨🏽 👨🏾‍❤️‍💋‍👨🏾 👨🏾‍❤️‍💋‍👨🏿 👨🏿‍❤️‍💋‍👨🏻 👨🏿‍❤️‍💋‍👨🏼 👨🏿‍❤️‍💋‍👨🏽 👨🏿‍❤️‍💋‍👨🏾 👨🏿‍❤️‍💋‍👨🏿 👩🏻‍❤️‍💋‍👨🏻 👩🏻‍❤️‍💋‍👨🏼 👩🏻‍❤️‍💋‍👨🏽 👩🏻‍❤️‍💋‍👨🏾 👩🏻‍❤️‍💋‍👨🏿 👩🏻‍❤️‍💋‍👩🏻 👩🏻‍❤️‍💋‍👩🏼 👩🏻‍❤️‍💋‍👩🏽 👩🏻‍❤️‍💋‍👩🏾 👩🏻‍❤️‍💋‍👩🏿 👩🏼‍❤️‍💋‍👨🏻 👩🏼‍❤️‍💋‍👨🏼 👩🏼‍❤️‍💋‍👨🏽 👩🏼‍❤️‍💋‍👨🏾 👩🏼‍❤️‍💋‍👨🏿 👩🏼‍❤️‍💋‍👩🏻 👩🏼‍❤️‍💋‍👩🏼 👩🏼‍❤️‍💋‍👩🏽 👩🏼‍❤️‍💋‍👩🏾 👩🏼‍❤️‍💋‍👩🏿 👩🏽‍❤️‍💋‍👨🏻 👩🏽‍❤️‍💋‍👨🏼 👩🏽‍❤️‍💋‍👨🏽 👩🏽‍❤️‍💋‍👨🏾 👩🏽‍❤️‍💋‍👨🏿 👩🏽‍❤️‍💋‍👩🏻 👩🏽‍❤️‍💋‍👩🏼 👩🏽‍❤️‍💋‍👩🏽 👩🏽‍❤️‍💋‍👩🏾 👩🏽‍❤️‍💋‍👩🏿 👩🏾‍❤️‍💋‍👨🏻 👩🏾‍❤️‍💋‍👨🏼 👩🏾‍❤️‍💋‍👨🏽 👩🏾‍❤️‍💋‍👨🏾 👩🏾‍❤️‍💋‍👨🏿 👩🏾‍❤️‍💋‍👩🏻 👩🏾‍❤️‍💋‍👩🏼 👩🏾‍❤️‍💋‍👩🏽 👩🏾‍❤️‍💋‍👩🏾 👩🏾‍❤️‍💋‍👩🏿 👩🏿‍❤️‍💋‍👨🏻 👩🏿‍❤️‍💋‍👨🏼 👩🏿‍❤️‍💋‍👨🏽 👩🏿‍❤️‍💋‍👨🏾 👩🏿‍❤️‍💋‍👨🏿 👩🏿‍❤️‍💋‍👩🏻 👩🏿‍❤️‍💋‍👩🏼 👩🏿‍❤️‍💋‍👩🏽 👩🏿‍❤️‍💋‍👩🏾 👩🏿‍❤️‍💋‍👩🏿 🧑🏻‍❤️‍💋‍🧑🏼 🧑🏻‍❤️‍💋‍🧑🏽 🧑🏻‍❤️‍💋‍🧑🏾 🧑🏻‍❤️‍💋‍🧑🏿 🧑🏼‍❤️‍💋‍🧑🏻 🧑🏼‍❤️‍💋‍🧑🏽 🧑🏼‍❤️‍💋‍🧑🏾 🧑🏼‍❤️‍💋‍🧑🏿 🧑🏽‍❤️‍💋‍🧑🏻 🧑🏽‍❤️‍💋‍🧑🏼 🧑🏽‍❤️‍💋‍🧑🏾 🧑🏽‍❤️‍💋‍🧑🏿 🧑🏾‍❤️‍💋‍🧑🏻 🧑🏾‍❤️‍💋‍🧑🏼 🧑🏾‍❤️‍💋‍🧑🏽 🧑🏾‍❤️‍💋‍🧑🏿 🧑🏿‍❤️‍💋‍🧑🏻 🧑🏿‍❤️‍💋‍🧑🏼 🧑🏿‍❤️‍💋‍🧑🏽 🧑🏿‍❤️‍💋‍🧑🏾 👭 🧑‍🤝‍🧑 👬 👫 👩‍❤️‍👩 💑 👨‍❤️‍👨 👩‍❤️‍👨 👩‍❤️‍💋‍👩 💏 👨‍❤️‍💋‍👨 👩‍❤️‍💋‍👨 👪 👨‍👩‍👦 👨‍👩‍👧 👨‍👩‍👧‍👦 👨‍👩‍👦‍👦 👨‍👩‍👧‍👧 👨‍👨‍👦 👨‍👨‍👧 👨‍👨‍👧‍👦 👨‍👨‍👦‍👦 👨‍👨‍👧‍👧 👩‍👩‍👦 👩‍👩‍👧 👩‍👩‍👧‍👦 👩‍👩‍👦‍👦 👩‍👩‍👧‍👧 👨‍👦 👨‍👦‍👦 👨‍👧 👨‍👧‍👦 👨‍👧‍👧 👩‍👦 👩‍👦‍👦 👩‍👧 👩‍👧‍👦 👩‍ 👧‍👧 💋\"\n",
    "emoji_list = emoji_str.split(' ')\n",
    "emoji_regex = '|'.join(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b2bd0fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "all_emoji = []\n",
    "for message in df.clean_message:\n",
    "    foundEmoji = re.findall(emoji_regex, message)\n",
    "    for emoji in foundEmoji:\n",
    "        all_emoji.append(emoji)\n",
    "\n",
    "print(len(all_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52dbef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('😂', 12), ('🤔', 9), ('😁', 9), ('👍', 7), ('👉', 6), ('🙄', 6), ('😀', 5), ('😉', 5), ('😎', 5), ('😊', 4), ('🤗', 3), ('😅', 3), ('👏', 3), ('❤️', 3), ('😥', 2), ('😵', 2), ('😍', 2), ('😆', 2), ('🤘', 2), ('🤬', 2), ('🙂', 2), ('😖', 1), ('🤥', 1), ('☝️', 1), ('🙁', 1), ('🤣', 1), ('😳', 1), ('🙌', 1), ('👌', 1), ('🤓', 1), ('😌', 1), ('🤢', 1), ('👇', 1), ('😜', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter=collections.Counter(all_emoji)\n",
    "print(counter.most_common(100))\n",
    "\n",
    "most_common_values= [word for word, word_count in counter.most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0f01c6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_values = ['😂',\n",
    " '😭',\n",
    " '🤣',\n",
    " '🙏',\n",
    " '🥺',\n",
    " '😩',\n",
    " '🤔',\n",
    " '🥰',\n",
    " '🥴',\n",
    " '👏',\n",
    " '😍',\n",
    " '🙄',\n",
    " '🙌',\n",
    " '💙',\n",
    " '😅',\n",
    " '💜',\n",
    " '💕',\n",
    " '😊',\n",
    " '👍',\n",
    " '😘',\n",
    " '😔',\n",
    " '💔',\n",
    " '😒',\n",
    " '🙃',\n",
    " '😎',\n",
    " '😉',\n",
    " '😁',\n",
    " '😢',\n",
    " '😌',\n",
    " '🖤',\n",
    " '😆',\n",
    " '🤗',\n",
    " '😳',\n",
    " '🥳',\n",
    " '✊',\n",
    " '😤',\n",
    " '💛',\n",
    " '😬',\n",
    " '🤬',\n",
    " '👇',\n",
    " '😡',\n",
    " '👌',\n",
    " '🤩',\n",
    " '💖',\n",
    " '😫',\n",
    " '💋',\n",
    " '\\U0001f90d',\n",
    " '💚',\n",
    " '💗',\n",
    " '🙂',\n",
    " '😞',\n",
    " '😷',\n",
    " '🤯',\n",
    " '😀',\n",
    " '😴',\n",
    " '😋',\n",
    " '😏',\n",
    " '🤞',\n",
    " '😜',\n",
    " '👉',\n",
    " '🤘',\n",
    " '😪',\n",
    " '🤤',\n",
    " '😑',\n",
    " '🤪',\n",
    " '😐',\n",
    " '😱',\n",
    " '😄',\n",
    " '👊',\n",
    " '🤢',\n",
    " '🤨',\n",
    " '🧐',\n",
    " '🥶',\n",
    " '😕',\n",
    " '💩',\n",
    " '😇',\n",
    " '🤮',\n",
    " '🥵',\n",
    " '🤧',\n",
    " '👋',\n",
    " '👎',\n",
    " '😖',\n",
    " '🤓',\n",
    " '😃',\n",
    " '\\U0001f971',\n",
    " '💞',\n",
    " '😓',\n",
    " '🤝',\n",
    " '🧡',\n",
    " '😹',\n",
    " '🖕',\n",
    " '😝',\n",
    " '😣',\n",
    " '🤫',\n",
    " '😰',\n",
    " '😥',\n",
    " '🤭',\n",
    " '💓',\n",
    " '😠',\n",
    " '😮']\n",
    "\n",
    "tokenizer.add_tokens(most_common_values, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5834e585",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-a59071f0e8c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlist_of_space_separated_pieces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_of_space_separated_pieces\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0munk_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munk_token_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0munknown_strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpiece\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_space_separated_pieces\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munk_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-a59071f0e8c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlist_of_space_separated_pieces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_of_space_separated_pieces\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0munk_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munk_token_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0munknown_strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpiece\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_space_separated_pieces\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munk_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2461\u001b[0m             )\n\u001b[0;32m   2462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2463\u001b[1;33m             return self.encode_plus(\n\u001b[0m\u001b[0;32m   2464\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2465\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2534\u001b[0m         )\n\u001b[0;32m   2535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2536\u001b[1;33m         return self._encode_plus(\n\u001b[0m\u001b[0;32m   2537\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2538\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         return self.prepare_for_model(\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mfirst_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[0mpair_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecond_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mprepare_for_model\u001b[1;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   3012\u001b[0m             \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"length\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3014\u001b[1;33m         batch_outputs = BatchEncoding(\n\u001b[0m\u001b[0;32m   3015\u001b[0m             \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3016\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_sequences\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     ):\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEncodingFast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\_collections_abc.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, other, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    833\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"keys\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, item)\u001b[0m\n\u001b[0;32m   1009\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#other UNK tokens\n",
    "\n",
    "# unk_tokens = []\n",
    "# for message in df.clean_message.values:\n",
    "#     list_of_space_separated_pieces = message.strip().split()\n",
    "#     ids = [tokenizer(piece, add_special_tokens=False)[\"input_ids\"] for piece in list_of_space_separated_pieces]\n",
    "#     unk_indices = [i for i, encoded in enumerate(ids) if tokenizer.unk_token_id in encoded]\n",
    "#     unknown_strings = [piece for i, piece in enumerate(list_of_space_separated_pieces) if i in unk_indices]\n",
    "#     for unk_str in unknown_strings:\n",
    "#         unk_tokens.append(unk_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collections\n",
    "\n",
    "# counter=collections.Counter(unk_tokens)\n",
    "# print(counter.most_common(100))\n",
    "\n",
    "# most_common_values= [word for word, word_count in counter.most_common(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504acea3",
   "metadata": {},
   "source": [
    "### Find max length for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6b28671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_lens = []\n",
    "for txt in list(df.clean_message.values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "    \n",
    "sns.displot(token_lens)\n",
    "plt.xlim([0, 125])\n",
    "plt.xlabel('Token count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b9e882ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6402cf",
   "metadata": {},
   "source": [
    "### Encode messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bb945e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_train[\"clean_message\"].values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].clean_message.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9c13eef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044, 304)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "582b69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset_train, './datasetsLowercase/dataset_train.pt')\n",
    "# torch.save(dataset_val, './datasetsLowercase/dataset_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "062b659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = torch.load('./datasetsLowercase/dataset_train.pt')\n",
    "# dataset_val = torch.load('./datasetsLowercase/dataset_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f2bfadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11af821",
   "metadata": {},
   "source": [
    "# Model \"lvBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d3a7ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../lvbert_pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./../lvbert_pytorch/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('./../lvbert_pytorch/',\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "92f447a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32104, 768)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c84e9bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('modelsEmoji-pnn/finetuned_lvBERT_epoch_1.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c906d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16189453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "61d95dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9bf40782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure weighted F1\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b5fc030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c05ccb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model. Returns average validation loss, predictions, true values\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_val, desc='Validating:', leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ef371",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "81fe5ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f454919bf21d46ae8c608a506e38cf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.0103731841752024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7596883773803711\n",
      "F1 Score (Weighted): 0.6781989132750907\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.62      0.69       145\n",
      "           1       0.51      0.74      0.61        62\n",
      "           2       0.70      0.71      0.71        97\n",
      "\n",
      "    accuracy                           0.67       304\n",
      "   macro avg       0.66      0.69      0.67       304\n",
      "weighted avg       0.70      0.67      0.68       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         90       34       21\n",
      "       positive         8       46        8\n",
      "       negative        18       10       69\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.6617295705910885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.749779099225998\n",
      "F1 Score (Weighted): 0.6782817488190377\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.60      0.68       145\n",
      "           1       0.55      0.85      0.67        62\n",
      "           2       0.67      0.68      0.68        97\n",
      "\n",
      "    accuracy                           0.68       304\n",
      "   macro avg       0.67      0.71      0.68       304\n",
      "weighted avg       0.70      0.68      0.68       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         87       30       28\n",
      "       positive         5       53        4\n",
      "       negative        18       13       66\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.498219811555111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7557681024074554\n",
      "F1 Score (Weighted): 0.6643161078098473\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.66       145\n",
      "           1       0.54      0.85      0.66        62\n",
      "           2       0.65      0.69      0.67        97\n",
      "\n",
      "    accuracy                           0.66       304\n",
      "   macro avg       0.66      0.70      0.66       304\n",
      "weighted avg       0.70      0.66      0.66       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         82       32       31\n",
      "       positive         4       53        5\n",
      "       negative        17       13       67\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.4007976705377752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7787293076515198\n",
      "F1 Score (Weighted): 0.668397731262742\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.57      0.67       145\n",
      "           1       0.54      0.85      0.66        62\n",
      "           2       0.66      0.69      0.67        97\n",
      "\n",
      "    accuracy                           0.67       304\n",
      "   macro avg       0.67      0.71      0.67       304\n",
      "weighted avg       0.70      0.67      0.67       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         83       32       30\n",
      "       positive         4       53        5\n",
      "       negative        16       14       67\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.3486848658684528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7553184241056442\n",
      "F1 Score (Weighted): 0.6913896981456139\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.70       145\n",
      "           1       0.58      0.85      0.69        62\n",
      "           2       0.67      0.70      0.68        97\n",
      "\n",
      "    accuracy                           0.69       304\n",
      "   macro avg       0.68      0.72      0.69       304\n",
      "weighted avg       0.72      0.69      0.69       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         89       27       29\n",
      "       positive         4       53        5\n",
      "       negative        17       12       68\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'modelsEmojiTwttrPNN/finetuned_lvBERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "    \n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    \n",
    "    print('Classification report:')\n",
    "    print(classification_report(true_vals, preds_flat))\n",
    "    print('Confusion matrix:')\n",
    "    print(pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "            index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "            columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']]))\n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271f1a9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b344990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelsEmoji/finetuned_BERT_epoch_X.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(true_vals, preds_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "        index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "        columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38twttr",
   "language": "python",
   "name": "venv38twttr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
