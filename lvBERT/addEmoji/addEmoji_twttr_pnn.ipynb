{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2616b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "66a84c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da5ece",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "47471ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsÄks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label\n",
       "0  1478404  Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...      0\n",
       "1  1478695  Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...      0\n",
       "2  1478812  airBaltic uzsÄks lidojumus uz diviem jauniem g...      0\n",
       "3  1479295  Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...      0\n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../../labeledTweets/allLabeledTweets.csv')\n",
    "df = df[['id', 'message', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82da84e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    968\n",
       "2    647\n",
       "1    410\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b78b2f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...</td>\n",
       "      <td>0</td>\n",
       "      <td>Augulis: airBaltic â€œpotenciÄlie pircÄ“ji ir no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsÄks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsÄks lidojumus uz diviem jauniem g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "      <td>MENTION MENTION MENTION NetrÄpÄ«s kÄdam AirBalt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label  \\\n",
       "0  1478404  Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...      0   \n",
       "1  1478695  Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...      0   \n",
       "2  1478812  airBaltic uzsÄks lidojumus uz diviem jauniem g...      0   \n",
       "3  1479295  Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...      0   \n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0   \n",
       "\n",
       "                                       clean_message  \n",
       "0  Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...  \n",
       "1  Augulis: airBaltic â€œpotenciÄlie pircÄ“ji ir no ...  \n",
       "2  airBaltic uzsÄks lidojumus uz diviem jauniem g...  \n",
       "3  Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...  \n",
       "4  MENTION MENTION MENTION NetrÄpÄ«s kÄdam AirBalt...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newLine =\"\\\\n|\\\\r\"\n",
    "urls = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "numbers = '\\d+((\\.|\\-)\\d+)?'\n",
    "mentions = '\\B\\@([\\w\\-]+)'\n",
    "hashtag = '#'\n",
    "whitespaces = '\\s+'\n",
    "leadTrailWhitespace = '^\\s+|\\s+?$'\n",
    "\n",
    "df['clean_message'] = df['message']\n",
    "df['clean_message'] = df['clean_message'].str.replace(newLine,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(urls,' URL ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(mentions,' MENTION ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(numbers,' NMBR ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(hashtag,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(whitespaces,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(leadTrailWhitespace,'',regex=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc0d24",
   "metadata": {},
   "source": [
    "# Train, validate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c1b2abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "051783f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  message  clean_message\n",
       "label data_type                             \n",
       "0     train      823      823            823\n",
       "      val        145      145            145\n",
       "1     train      348      348            348\n",
       "      val         62       62             62\n",
       "2     train      550      550            550\n",
       "      val         97       97             97"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6440d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...</td>\n",
       "      <td>0</td>\n",
       "      <td>Augulis: airBaltic â€œpotenciÄlie pircÄ“ji ir no ...</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsÄks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsÄks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "      <td>MENTION MENTION MENTION NetrÄpÄ«s kÄdam AirBalt...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label  \\\n",
       "0  1478404  Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...      0   \n",
       "1  1478695  Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...      0   \n",
       "2  1478812  airBaltic uzsÄks lidojumus uz diviem jauniem g...      0   \n",
       "3  1479295  Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...      0   \n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0   \n",
       "\n",
       "                                       clean_message data_type  \n",
       "0  Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...     train  \n",
       "1  Augulis: airBaltic â€œpotenciÄlie pircÄ“ji ir no ...       val  \n",
       "2  airBaltic uzsÄks lidojumus uz diviem jauniem g...     train  \n",
       "3  Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...     train  \n",
       "4  MENTION MENTION MENTION NetrÄpÄ«s kÄdam AirBalt...     train  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "63617ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./tweets_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb2b33",
   "metadata": {},
   "source": [
    "## Balance training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1d625ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    823\n",
       "2    550\n",
       "1    348\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.data_type=='train']['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13204ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    348\n",
       "1    348\n",
       "2    348\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = [df[df.data_type=='train'].clean_message, df[df.data_type=='train'].label]\n",
    "df_train = pd.concat(df_train, axis=1, keys=[\"clean_message\", \"label\"])\n",
    "\n",
    "df_0 = df_train[df_train['label']==0]\n",
    "df_1 = df_train[df_train['label']==1]\n",
    "df_2 = df_train[df_train['label']==2]\n",
    "\n",
    "df_0_downsampled = df_0.sample(df_1.shape[0], random_state=42)\n",
    "df_2_downsampled = df_2.sample(df_1.shape[0], random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_0_downsampled, df_2_downsampled, df_1])\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c86ffe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Å odien norisinÄs SEB MTB maratons. FiniÅ¡s MENT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>MENTION Sveiks! Tas neattiecas uz mÅ«su ekspert...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>MENTION MENTION Starpcitu, Maxima tirgo labus ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>TaisnÄ«ba ir uzvarÄ“jusi! Martins Dukurs kÄ¼Å«st p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>MENTION Ğ¡Ğ¿Ğ°ÑĞ¸Ğ±Ğ¾ Ğ·Ğ° Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ.^el</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_message  label\n",
       "700   Å odien norisinÄs SEB MTB maratons. FiniÅ¡s MENT...      0\n",
       "1911  MENTION Sveiks! Tas neattiecas uz mÅ«su ekspert...      1\n",
       "505   MENTION MENTION Starpcitu, Maxima tirgo labus ...      1\n",
       "1471  TaisnÄ«ba ir uzvarÄ“jusi! Martins Dukurs kÄ¼Å«st p...      1\n",
       "1537                      MENTION Ğ¡Ğ¿Ğ°ÑĞ¸Ğ±Ğ¾ Ğ·Ğ° Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ.^el      0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle rows\n",
    "import sklearn\n",
    "\n",
    "df_train = sklearn.utils.shuffle(df_train, random_state=0)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843c4ae",
   "metadata": {},
   "source": [
    "# Tokenizer \"lvBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "43b43559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./../lvbert_pytorch/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bffc0",
   "metadata": {},
   "source": [
    "## Add emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ade333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoji_str = \"ğŸ˜€ ğŸ˜ƒ ğŸ˜„ ğŸ˜ ğŸ˜† ğŸ˜… ğŸ˜‚ ğŸ¤£ ğŸ˜Š ğŸ˜‡ ğŸ™‚ ğŸ™ƒ ğŸ˜‰ ğŸ˜Œ ğŸ˜ ğŸ¥° ğŸ˜˜ ğŸ˜— ğŸ˜™ ğŸ˜š ğŸ˜‹ ğŸ˜› ğŸ˜ ğŸ˜œ ğŸ¤ª ğŸ¤¨ ğŸ§ ğŸ¤“ ğŸ˜ ğŸ¤© ğŸ¥³ ğŸ˜ ğŸ˜’ ğŸ˜ ğŸ˜” ğŸ˜Ÿ ğŸ˜• ğŸ™ â˜¹ï¸ ğŸ˜£ ğŸ˜– ğŸ˜« ğŸ˜© ğŸ¥º ğŸ˜¢ ğŸ˜­ ğŸ˜¤ ğŸ˜  ğŸ˜¡ ğŸ¤¬ ğŸ¤¯ ğŸ˜³ ğŸ¥µ ğŸ¥¶ ğŸ˜± ğŸ˜¨ ğŸ˜° ğŸ˜¥ ğŸ˜“ ğŸ¤— ğŸ¤” ğŸ¤­ ğŸ¤« ğŸ¤¥ ğŸ˜¶ ğŸ˜ ğŸ˜‘ ğŸ˜¬ ğŸ™„ ğŸ˜¯ ğŸ˜¦ ğŸ˜§ ğŸ˜® ğŸ˜² ğŸ¥± ğŸ˜´ ğŸ¤¤ ğŸ˜ª ğŸ˜µ ğŸ¤ ğŸ¥´ ğŸ¤¢ ğŸ¤® ğŸ¤§ ğŸ˜· ğŸ¤’ ğŸ¤• ğŸ˜º ğŸ˜¸ ğŸ˜¹ ğŸ˜» ğŸ˜¼ ğŸ˜½ ğŸ™€ ğŸ˜¿ ğŸ˜¾ ğŸ’© ğŸ‘‹ ğŸ¤š ğŸ– âœ‹ ğŸ–– ğŸ‘Œ ğŸ¤ âœŒï¸ ğŸ¤ ğŸ¤Ÿ ğŸ¤˜ ğŸ¤™ ğŸ‘ˆ ğŸ‘‰ ğŸ‘† ğŸ–• ğŸ‘‡ â˜ï¸ ğŸ‘ ğŸ‘ âœŠ ğŸ‘Š ğŸ¤› ğŸ¤œ ğŸ‘ ğŸ™Œ ğŸ‘ ğŸ¤² ğŸ¤ ğŸ™'ğŸ‘‹ğŸ» ğŸ¤šğŸ» ğŸ–ğŸ» âœ‹ğŸ» ğŸ––ğŸ» ğŸ‘ŒğŸ» ğŸ¤ğŸ» âœŒğŸ» ğŸ¤ğŸ» ğŸ¤ŸğŸ» ğŸ¤˜ğŸ» ğŸ¤™ğŸ» ğŸ‘ˆğŸ» ğŸ‘‰ğŸ» ğŸ‘†ğŸ» ğŸ–•ğŸ» ğŸ‘‡ğŸ» â˜ğŸ» ğŸ‘ğŸ» ğŸ‘ğŸ» âœŠğŸ» ğŸ‘ŠğŸ» ğŸ¤›ğŸ» ğŸ¤œğŸ» ğŸ‘ğŸ» ğŸ™ŒğŸ» ğŸ‘ğŸ» ğŸ¤²ğŸ» ğŸ™ğŸ» ğŸ‘‹ğŸ¼ ğŸ¤šğŸ¼ ğŸ–ğŸ¼ âœ‹ğŸ¼ ğŸ––ğŸ¼ ğŸ‘ŒğŸ¼ ğŸ¤ğŸ¼ âœŒğŸ¼ ğŸ¤ğŸ¼ ğŸ¤ŸğŸ¼ ğŸ¤˜ğŸ¼ ğŸ¤™ğŸ¼ ğŸ‘ˆğŸ¼ ğŸ‘‰ğŸ¼ ğŸ‘†ğŸ¼ ğŸ–•ğŸ¼ ğŸ‘‡ğŸ¼ â˜ğŸ¼ ğŸ‘ğŸ¼ ğŸ‘ğŸ¼ âœŠğŸ¼ ğŸ‘ŠğŸ¼ ğŸ¤›ğŸ¼ ğŸ¤œğŸ¼ ğŸ‘ğŸ¼ ğŸ™ŒğŸ¼ ğŸ‘ğŸ¼ ğŸ¤²ğŸ¼ ğŸ™ğŸ¼ ğŸ‘‹ğŸ½ ğŸ¤šğŸ½ ğŸ–ğŸ½ âœ‹ğŸ½ ğŸ––ğŸ½ ğŸ‘ŒğŸ½ ğŸ¤ğŸ½ âœŒğŸ½ ğŸ¤ğŸ½ ğŸ¤ŸğŸ½ ğŸ¤˜ğŸ½ ğŸ¤™ğŸ½ ğŸ‘ˆğŸ½ ğŸ‘‰ğŸ½ ğŸ‘†ğŸ½ ğŸ–•ğŸ½ ğŸ‘‡ğŸ½ â˜ğŸ½ ğŸ‘ğŸ½ ğŸ‘ğŸ½ âœŠğŸ½ ğŸ‘ŠğŸ½ ğŸ¤›ğŸ½ ğŸ¤œğŸ½ ğŸ‘ğŸ½ ğŸ™ŒğŸ½ ğŸ‘ğŸ½ ğŸ¤²ğŸ½ ğŸ™ğŸ‘‹ğŸ¾ ğŸ¤šğŸ¾ ğŸ–ğŸ¾ âœ‹ğŸ¾ ğŸ––ğŸ¾ ğŸ‘ŒğŸ¾ ğŸ¤ğŸ¾ âœŒğŸ¾ ğŸ¤ğŸ¾ ğŸ¤ŸğŸ¾ ğŸ¤˜ğŸ¾ ğŸ¤™ğŸ¾ ğŸ‘ˆğŸ¾ ğŸ‘‰ğŸ¾ ğŸ‘†ğŸ¾ ğŸ–•ğŸ¾ ğŸ‘‡ğŸ¾ â˜ğŸ¾ ğŸ‘ğŸ¾ ğŸ‘ğŸ¾ âœŠğŸ¾ ğŸ‘ŠğŸ¾ ğŸ¤›ğŸ¾ ğŸ¤œğŸ¾ ğŸ‘ğŸ¾ ğŸ™ŒğŸ¾ ğŸ‘ğŸ¾ ğŸ¤²ğŸ¾ ğŸ™ ğŸ‘‹ğŸ¿ ğŸ¤šğŸ¿ ğŸ–ğŸ¿ âœ‹ğŸ¿ ğŸ––ğŸ¿ ğŸ‘ŒğŸ¿ ğŸ¤ğŸ¿ âœŒğŸ¿ ğŸ¤ğŸ¿ ğŸ¤ŸğŸ¿ ğŸ¤˜ğŸ¿ ğŸ¤™ğŸ¿ ğŸ‘ˆğŸ¿ ğŸ‘‰ğŸ¿ ğŸ‘†ğŸ¿ ğŸ–•ğŸ¿ ğŸ‘‡ğŸ¿ â˜ğŸ¿ ğŸ‘ğŸ¿ ğŸ‘ğŸ¿ âœŠğŸ¿ ğŸ‘ŠğŸ¿ ğŸ¤›ğŸ¿ ğŸ¤œğŸ¿ ğŸ‘ğŸ¿ ğŸ™ŒğŸ¿ ğŸ‘ğŸ¿ ğŸ¤²ğŸ¿ ğŸ™ğŸ¿ â¤ï¸ ğŸ§¡ ğŸ’› ğŸ’š ğŸ’™ ğŸ’œ ğŸ–¤ ğŸ¤ ğŸ¤ ğŸ’” â£ï¸ ğŸ’• ğŸ’ ğŸ’“ ğŸ’— ğŸ’– ğŸ’˜ ğŸ’ ğŸ’Ÿ ğŸ’‘ğŸ» ğŸ’‘ğŸ¼ ğŸ’‘ğŸ½ ğŸ’‘ğŸ¾ ğŸ’‘ğŸ¿ ğŸ’ğŸ» ğŸ’ğŸ¼ ğŸ’ğŸ½ ğŸ’ğŸ¾ ğŸ’ğŸ¿ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ‘­ ğŸ§‘â€ğŸ¤â€ğŸ§‘ ğŸ‘¬ ğŸ‘« ğŸ‘©â€â¤ï¸â€ğŸ‘© ğŸ’‘ ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘© ğŸ’ ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ ğŸ‘ª ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ ğŸ‘§â€ğŸ‘§ ğŸ’‹\"\n",
    "emoji_list = emoji_str.split(' ')\n",
    "emoji_regex = '|'.join(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b2bd0fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "all_emoji = []\n",
    "for message in df.clean_message:\n",
    "    foundEmoji = re.findall(emoji_regex, message)\n",
    "    for emoji in foundEmoji:\n",
    "        all_emoji.append(emoji)\n",
    "\n",
    "print(len(all_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52dbef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ğŸ˜‚', 12), ('ğŸ¤”', 9), ('ğŸ˜', 9), ('ğŸ‘', 7), ('ğŸ‘‰', 6), ('ğŸ™„', 6), ('ğŸ˜€', 5), ('ğŸ˜‰', 5), ('ğŸ˜', 5), ('ğŸ˜Š', 4), ('ğŸ¤—', 3), ('ğŸ˜…', 3), ('ğŸ‘', 3), ('â¤ï¸', 3), ('ğŸ˜¥', 2), ('ğŸ˜µ', 2), ('ğŸ˜', 2), ('ğŸ˜†', 2), ('ğŸ¤˜', 2), ('ğŸ¤¬', 2), ('ğŸ™‚', 2), ('ğŸ˜–', 1), ('ğŸ¤¥', 1), ('â˜ï¸', 1), ('ğŸ™', 1), ('ğŸ¤£', 1), ('ğŸ˜³', 1), ('ğŸ™Œ', 1), ('ğŸ‘Œ', 1), ('ğŸ¤“', 1), ('ğŸ˜Œ', 1), ('ğŸ¤¢', 1), ('ğŸ‘‡', 1), ('ğŸ˜œ', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter=collections.Counter(all_emoji)\n",
    "print(counter.most_common(100))\n",
    "\n",
    "most_common_values= [word for word, word_count in counter.most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0f01c6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_values = ['ğŸ˜‚',\n",
    " 'ğŸ˜­',\n",
    " 'ğŸ¤£',\n",
    " 'ğŸ™',\n",
    " 'ğŸ¥º',\n",
    " 'ğŸ˜©',\n",
    " 'ğŸ¤”',\n",
    " 'ğŸ¥°',\n",
    " 'ğŸ¥´',\n",
    " 'ğŸ‘',\n",
    " 'ğŸ˜',\n",
    " 'ğŸ™„',\n",
    " 'ğŸ™Œ',\n",
    " 'ğŸ’™',\n",
    " 'ğŸ˜…',\n",
    " 'ğŸ’œ',\n",
    " 'ğŸ’•',\n",
    " 'ğŸ˜Š',\n",
    " 'ğŸ‘',\n",
    " 'ğŸ˜˜',\n",
    " 'ğŸ˜”',\n",
    " 'ğŸ’”',\n",
    " 'ğŸ˜’',\n",
    " 'ğŸ™ƒ',\n",
    " 'ğŸ˜',\n",
    " 'ğŸ˜‰',\n",
    " 'ğŸ˜',\n",
    " 'ğŸ˜¢',\n",
    " 'ğŸ˜Œ',\n",
    " 'ğŸ–¤',\n",
    " 'ğŸ˜†',\n",
    " 'ğŸ¤—',\n",
    " 'ğŸ˜³',\n",
    " 'ğŸ¥³',\n",
    " 'âœŠ',\n",
    " 'ğŸ˜¤',\n",
    " 'ğŸ’›',\n",
    " 'ğŸ˜¬',\n",
    " 'ğŸ¤¬',\n",
    " 'ğŸ‘‡',\n",
    " 'ğŸ˜¡',\n",
    " 'ğŸ‘Œ',\n",
    " 'ğŸ¤©',\n",
    " 'ğŸ’–',\n",
    " 'ğŸ˜«',\n",
    " 'ğŸ’‹',\n",
    " '\\U0001f90d',\n",
    " 'ğŸ’š',\n",
    " 'ğŸ’—',\n",
    " 'ğŸ™‚',\n",
    " 'ğŸ˜',\n",
    " 'ğŸ˜·',\n",
    " 'ğŸ¤¯',\n",
    " 'ğŸ˜€',\n",
    " 'ğŸ˜´',\n",
    " 'ğŸ˜‹',\n",
    " 'ğŸ˜',\n",
    " 'ğŸ¤',\n",
    " 'ğŸ˜œ',\n",
    " 'ğŸ‘‰',\n",
    " 'ğŸ¤˜',\n",
    " 'ğŸ˜ª',\n",
    " 'ğŸ¤¤',\n",
    " 'ğŸ˜‘',\n",
    " 'ğŸ¤ª',\n",
    " 'ğŸ˜',\n",
    " 'ğŸ˜±',\n",
    " 'ğŸ˜„',\n",
    " 'ğŸ‘Š',\n",
    " 'ğŸ¤¢',\n",
    " 'ğŸ¤¨',\n",
    " 'ğŸ§',\n",
    " 'ğŸ¥¶',\n",
    " 'ğŸ˜•',\n",
    " 'ğŸ’©',\n",
    " 'ğŸ˜‡',\n",
    " 'ğŸ¤®',\n",
    " 'ğŸ¥µ',\n",
    " 'ğŸ¤§',\n",
    " 'ğŸ‘‹',\n",
    " 'ğŸ‘',\n",
    " 'ğŸ˜–',\n",
    " 'ğŸ¤“',\n",
    " 'ğŸ˜ƒ',\n",
    " '\\U0001f971',\n",
    " 'ğŸ’',\n",
    " 'ğŸ˜“',\n",
    " 'ğŸ¤',\n",
    " 'ğŸ§¡',\n",
    " 'ğŸ˜¹',\n",
    " 'ğŸ–•',\n",
    " 'ğŸ˜',\n",
    " 'ğŸ˜£',\n",
    " 'ğŸ¤«',\n",
    " 'ğŸ˜°',\n",
    " 'ğŸ˜¥',\n",
    " 'ğŸ¤­',\n",
    " 'ğŸ’“',\n",
    " 'ğŸ˜ ',\n",
    " 'ğŸ˜®']\n",
    "\n",
    "tokenizer.add_tokens(most_common_values, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5834e585",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-a59071f0e8c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlist_of_space_separated_pieces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_of_space_separated_pieces\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0munk_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munk_token_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0munknown_strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpiece\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_space_separated_pieces\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munk_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-a59071f0e8c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlist_of_space_separated_pieces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_of_space_separated_pieces\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0munk_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munk_token_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0munknown_strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpiece\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpiece\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_space_separated_pieces\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munk_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2461\u001b[0m             )\n\u001b[0;32m   2462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2463\u001b[1;33m             return self.encode_plus(\n\u001b[0m\u001b[0;32m   2464\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2465\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2534\u001b[0m         )\n\u001b[0;32m   2535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2536\u001b[1;33m         return self._encode_plus(\n\u001b[0m\u001b[0;32m   2537\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2538\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         return self.prepare_for_model(\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mfirst_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[0mpair_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecond_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mprepare_for_model\u001b[1;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   3012\u001b[0m             \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"length\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3014\u001b[1;33m         batch_outputs = BatchEncoding(\n\u001b[0m\u001b[0;32m   3015\u001b[0m             \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3016\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_sequences\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     ):\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEncodingFast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\_collections_abc.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, other, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    833\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"keys\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, item)\u001b[0m\n\u001b[0;32m   1009\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#other UNK tokens\n",
    "\n",
    "# unk_tokens = []\n",
    "# for message in df.clean_message.values:\n",
    "#     list_of_space_separated_pieces = message.strip().split()\n",
    "#     ids = [tokenizer(piece, add_special_tokens=False)[\"input_ids\"] for piece in list_of_space_separated_pieces]\n",
    "#     unk_indices = [i for i, encoded in enumerate(ids) if tokenizer.unk_token_id in encoded]\n",
    "#     unknown_strings = [piece for i, piece in enumerate(list_of_space_separated_pieces) if i in unk_indices]\n",
    "#     for unk_str in unknown_strings:\n",
    "#         unk_tokens.append(unk_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collections\n",
    "\n",
    "# counter=collections.Counter(unk_tokens)\n",
    "# print(counter.most_common(100))\n",
    "\n",
    "# most_common_values= [word for word, word_count in counter.most_common(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504acea3",
   "metadata": {},
   "source": [
    "### Find max length for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6b28671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_lens = []\n",
    "for txt in list(df.clean_message.values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "    \n",
    "sns.displot(token_lens)\n",
    "plt.xlim([0, 125])\n",
    "plt.xlabel('Token count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b9e882ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6402cf",
   "metadata": {},
   "source": [
    "### Encode messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bb945e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_train[\"clean_message\"].values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].clean_message.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9c13eef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044, 304)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "582b69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset_train, './datasetsLowercase/dataset_train.pt')\n",
    "# torch.save(dataset_val, './datasetsLowercase/dataset_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "062b659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = torch.load('./datasetsLowercase/dataset_train.pt')\n",
    "# dataset_val = torch.load('./datasetsLowercase/dataset_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f2bfadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11af821",
   "metadata": {},
   "source": [
    "# Model \"lvBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d3a7ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./../lvbert_pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./../lvbert_pytorch/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('./../lvbert_pytorch/',\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "92f447a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32104, 768)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c84e9bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('modelsEmoji-pnn/finetuned_lvBERT_epoch_1.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c906d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16189453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "61d95dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9bf40782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure weighted F1\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b5fc030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c05ccb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model. Returns average validation loss, predictions, true values\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_val, desc='Validating:', leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ef371",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "81fe5ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f454919bf21d46ae8c608a506e38cf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.0103731841752024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7596883773803711\n",
      "F1 Score (Weighted): 0.6781989132750907\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.62      0.69       145\n",
      "           1       0.51      0.74      0.61        62\n",
      "           2       0.70      0.71      0.71        97\n",
      "\n",
      "    accuracy                           0.67       304\n",
      "   macro avg       0.66      0.69      0.67       304\n",
      "weighted avg       0.70      0.67      0.68       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         90       34       21\n",
      "       positive         8       46        8\n",
      "       negative        18       10       69\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.6617295705910885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.749779099225998\n",
      "F1 Score (Weighted): 0.6782817488190377\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.60      0.68       145\n",
      "           1       0.55      0.85      0.67        62\n",
      "           2       0.67      0.68      0.68        97\n",
      "\n",
      "    accuracy                           0.68       304\n",
      "   macro avg       0.67      0.71      0.68       304\n",
      "weighted avg       0.70      0.68      0.68       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         87       30       28\n",
      "       positive         5       53        4\n",
      "       negative        18       13       66\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.498219811555111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7557681024074554\n",
      "F1 Score (Weighted): 0.6643161078098473\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.66       145\n",
      "           1       0.54      0.85      0.66        62\n",
      "           2       0.65      0.69      0.67        97\n",
      "\n",
      "    accuracy                           0.66       304\n",
      "   macro avg       0.66      0.70      0.66       304\n",
      "weighted avg       0.70      0.66      0.66       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         82       32       31\n",
      "       positive         4       53        5\n",
      "       negative        17       13       67\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.4007976705377752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7787293076515198\n",
      "F1 Score (Weighted): 0.668397731262742\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.57      0.67       145\n",
      "           1       0.54      0.85      0.66        62\n",
      "           2       0.66      0.69      0.67        97\n",
      "\n",
      "    accuracy                           0.67       304\n",
      "   macro avg       0.67      0.71      0.67       304\n",
      "weighted avg       0.70      0.67      0.67       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         83       32       30\n",
      "       positive         4       53        5\n",
      "       negative        16       14       67\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.3486848658684528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7553184241056442\n",
      "F1 Score (Weighted): 0.6913896981456139\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.70       145\n",
      "           1       0.58      0.85      0.69        62\n",
      "           2       0.67      0.70      0.68        97\n",
      "\n",
      "    accuracy                           0.69       304\n",
      "   macro avg       0.68      0.72      0.69       304\n",
      "weighted avg       0.72      0.69      0.69       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         89       27       29\n",
      "       positive         4       53        5\n",
      "       negative        17       12       68\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'modelsEmojiTwttrPNN/finetuned_lvBERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "    \n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    \n",
    "    print('Classification report:')\n",
    "    print(classification_report(true_vals, preds_flat))\n",
    "    print('Confusion matrix:')\n",
    "    print(pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "            index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "            columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']]))\n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271f1a9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b344990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelsEmoji/finetuned_BERT_epoch_X.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(true_vals, preds_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "        index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "        columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38twttr",
   "language": "python",
   "name": "venv38twttr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
