{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2616b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "     -------------------------------------- 98.2/98.2 KB 200.8 kB/s eta 0:00:00\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.1-py2.py3-none-any.whl (234 kB)\n",
      "     -------------------------------------- 234.6/234.6 KB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (8.2.0)\n",
      "Requirement already satisfied: tensorflow-text~=2.8.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (2.8.1)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 6.2 MB/s eta 0:00:00\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 90.7/90.7 KB ? eta 0:00:00\n",
      "Requirement already satisfied: Cython in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (1.20.1)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "     ------------------------------------- 352.1/352.1 KB 10.7 MB/s eta 0:00:00\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.16.1-cp38-cp38-win_amd64.whl (755 kB)\n",
      "     ------------------------------------- 755.7/755.7 KB 24.0 MB/s eta 0:00:00\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 13.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (1.6.2)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "     ---------------------------------------- 59.0/59.0 KB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tensorflow~=2.8.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (2.8.0)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Using cached py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (3.3.4)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (1.2.4)\n",
      "Requirement already satisfied: six in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (1.15.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     -------------------------------------- 43.6/43.6 KB 427.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.4.tar.gz (106 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.3/61.3 KB ? eta 0:00:00\n",
      "Requirement already satisfied: pyyaml<6.0,>=5.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (5.4.1)\n",
      "Requirement already satisfied: psutil>=5.4.3 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (5.8.0)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.5.5.64-cp36-abi3-win_amd64.whl (35.3 MB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tf-models-official) (0.12.0)\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Using cached google_api_python_client-2.41.0-py2.py3-none-any.whl (8.3 MB)\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.7.1-py3-none-any.whl (114 kB)\n",
      "     -------------------------------------- 114.7/114.7 KB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.6.0)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 KB 5.4 MB/s eta 0:00:00\n",
      "Collecting google-auth-httplib2>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (4.59.0)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-6.1.1-py2.py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from pandas>=0.22.0->tf-models-official) (2021.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (1.44.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (2.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (1.0.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (2.8.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (1.6.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (13.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (2.10.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (0.5.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (1.1.2)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (52.0.0.post20210125)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (0.24.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorflow~=2.8.0->tf-models-official) (3.19.4)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.6-cp38-cp38-win_amd64.whl (75 kB)\n",
      "     ---------------------------------------- 75.1/75.1 KB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from matplotlib->tf-models-official) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from matplotlib->tf-models-official) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from matplotlib->tf-models-official) (2.4.7)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from oauth2client->tf-models-official) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from oauth2client->tf-models-official) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from oauth2client->tf-models-official) (0.4.8)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from sacrebleu->tf-models-official) (2021.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from sacrebleu->tf-models-official) (0.4.4)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from seqeval->tf-models-official) (0.24.1)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.7.0-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.8/48.8 KB 2.4 MB/s eta 0:00:00\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.9/86.9 KB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official) (0.36.2)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.56.0-py2.py3-none-any.whl (241 kB)\n",
      "     ------------------------------------- 241.5/241.5 KB 14.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (5.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from requests->kaggle>=1.3.9->tf-models-official) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from requests->kaggle>=1.3.9->tf-models-official) (2.0.12)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (2.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (0.6.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from importlib-resources->tensorflow-datasets->tf-models-official) (3.4.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from portalocker->sacrebleu->tf-models-official) (227)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "     -------------------------------------- 78.2/78.2 KB 870.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (4.11.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\aligo\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.2.0)\n",
      "Building wheels for collected packages: kaggle, py-cpuinfo, pycocotools, seqeval, promise\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=f210c6c8da09b9a041945e88c5e3692922c2d789dab7f76950a1ef21361a895b\n",
      "  Stored in directory: c:\\users\\aligo\\appdata\\local\\pip\\cache\\wheels\\29\\da\\11\\144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "  Building wheel for py-cpuinfo (setup.py): started\n",
      "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22245 sha256=dbc564d8204fb59ad257e81fc5286035789f492641c9a1e58826905f1fa7e9e8\n",
      "  Stored in directory: c:\\users\\aligo\\appdata\\local\\pip\\cache\\wheels\\57\\cb\\6d\\bab2257f26c5be4a96ff65c3d2a7122c96529b73773ee37f36\n",
      "  Building wheel for pycocotools (pyproject.toml): started\n",
      "  Building wheel for pycocotools (pyproject.toml): still running...\n",
      "  Building wheel for pycocotools (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp38-cp38-win_amd64.whl size=75156 sha256=b66d9f4e9ad25f8b0b3a2f4fc357e7479ebfe01e38841fabd6f2601174cfe12b\n",
      "  Stored in directory: c:\\users\\aligo\\appdata\\local\\pip\\cache\\wheels\\dd\\e2\\43\\3e93cd653b3346b3d702bb0509bc611189f95d60407bff1484\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=207e9d2a593b121a9aebf7d297afc639a9c6d7b79856e6370392996f26ea8c62\n",
      "  Stored in directory: c:\\users\\aligo\\appdata\\local\\pip\\cache\\wheels\\ad\\5c\\ba\\05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=1fa583a16a71155e28cb8a862348b2d5c66ddc2c0b60754c50056f372447d31a\n",
      "  Stored in directory: c:\\users\\aligo\\appdata\\local\\pip\\cache\\wheels\\54\\aa\\01\\724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built kaggle py-cpuinfo pycocotools seqeval promise\n",
      "Installing collected packages: text-unidecode, tabulate, sentencepiece, py-cpuinfo, gin-config, uritemplate, typeguard, python-slugify, promise, portalocker, opencv-python-headless, importlib-resources, httplib2, googleapis-common-protos, dm-tree, dill, tf-slim, tensorflow-model-optimization, tensorflow-metadata, tensorflow-addons, sacrebleu, oauth2client, kaggle, tensorflow-datasets, seqeval, pycocotools, google-auth-httplib2, google-api-core, google-api-python-client, tf-models-official\n",
      "Successfully installed dill-0.3.4 dm-tree-0.1.6 gin-config-0.5.0 google-api-core-2.7.1 google-api-python-client-2.41.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.56.0 httplib2-0.20.4 importlib-resources-5.4.0 kaggle-1.5.12 oauth2client-4.1.3 opencv-python-headless-4.5.5.64 portalocker-2.4.0 promise-2.3 py-cpuinfo-8.0.0 pycocotools-2.0.4 python-slugify-6.1.1 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tabulate-0.8.9 tensorflow-addons-0.16.1 tensorflow-datasets-4.5.2 tensorflow-metadata-1.7.0 tensorflow-model-optimization-0.7.1 text-unidecode-1.3 tf-models-official-2.8.0 tf-slim-1.1.0 typeguard-2.13.3 uritemplate-4.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\__main__.py\", line 31, in <module>\n",
      "    sys.exit(_main())\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\contextlib.py\", line 120, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\command_context.py\", line 20, in main_context\n",
      "    yield\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\contextlib.py\", line 525, in __exit__\n",
      "    raise exc_details[1]\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\temp_dir.py\", line 70, in tempdir_registry\n",
      "    yield _tempdir_registry\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\contextlib.py\", line 510, in __exit__\n",
      "    if cb(*exc_details):\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\contextlib.py\", line 120, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\temp_dir.py\", line 36, in global_tempdir_manager\n",
      "    _tempdir_manager = old_tempdir_manager\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\contextlib.py\", line 525, in __exit__\n",
      "    raise exc_details[1]\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\contextlib.py\", line 510, in __exit__\n",
      "    if cb(*exc_details):\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\temp_dir.py\", line 156, in __exit__\n",
      "    self.cleanup()\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\temp_dir.py\", line 173, in cleanup\n",
      "    rmtree(self._path)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 326, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 406, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 362, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 195, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 432, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 409, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 124, in rmtree\n",
      "    shutil.rmtree(dir, ignore_errors=ignore_errors, onerror=rmtree_errorhandler)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\shutil.py\", line 740, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\shutil.py\", line 613, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\shutil.py\", line 613, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\shutil.py\", line 613, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\shutil.py\", line 618, in _rmtree_unsafe\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"C:\\Users\\aligo\\anaconda3\\lib\\shutil.py\", line 616, in _rmtree_unsafe\n",
      "    os.unlink(fullname)\n",
      "PermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\aligo\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-pro574h8\\\\overlay\\\\Lib\\\\site-packages\\\\numpy\\\\.libs\\\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66a84c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da5ece",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47471ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic “potenciālie pircēji ir no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label\n",
       "0  1478404  Tiek vērtēti trīs potenciālie airBaltic invest...      0\n",
       "1  1478695  Augulis: #airBaltic “potenciālie pircēji ir no...      0\n",
       "2  1478812  airBaltic uzsāks lidojumus uz diviem jauniem g...      0\n",
       "3  1479295  Ministrs: Sarunas turpinās ar trīs potenciālaj...      0\n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../../labeledTweets/allLabeledTweets.csv')\n",
    "df = df[['id', 'message', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7cf93da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    968\n",
       "2    647\n",
       "1    410\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42acbbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic “potenciālie pircēji ir no...</td>\n",
       "      <td>0</td>\n",
       "      <td>Augulis: #airBaltic “potenciālie pircēji ir no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label  \\\n",
       "0  1478404  Tiek vērtēti trīs potenciālie airBaltic invest...      0   \n",
       "1  1478695  Augulis: #airBaltic “potenciālie pircēji ir no...      0   \n",
       "2  1478812  airBaltic uzsāks lidojumus uz diviem jauniem g...      0   \n",
       "3  1479295  Ministrs: Sarunas turpinās ar trīs potenciālaj...      0   \n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0   \n",
       "\n",
       "                                       clean_message  \n",
       "0  Tiek vērtēti trīs potenciālie airBaltic invest...  \n",
       "1  Augulis: #airBaltic “potenciālie pircēji ir no...  \n",
       "2  airBaltic uzsāks lidojumus uz diviem jauniem g...  \n",
       "3  Ministrs: Sarunas turpinās ar trīs potenciālaj...  \n",
       "4  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newLine =\"\\\\n|\\\\r\"\n",
    "df['clean_message'] = df['message'].str.replace(newLine,' ',regex=True).str.strip()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc0d24",
   "metadata": {},
   "source": [
    "# Train, validate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1b2abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "051783f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  message  clean_message\n",
       "label data_type                             \n",
       "0     train      823      823            823\n",
       "      val        145      145            145\n",
       "1     train      348      348            348\n",
       "      val         62       62             62\n",
       "2     train      550      550            550\n",
       "      val         97       97             97"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6440d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic “potenciālie pircēji ir no...</td>\n",
       "      <td>0</td>\n",
       "      <td>Augulis: #airBaltic “potenciālie pircēji ir no...</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label  \\\n",
       "0  1478404  Tiek vērtēti trīs potenciālie airBaltic invest...      0   \n",
       "1  1478695  Augulis: #airBaltic “potenciālie pircēji ir no...      0   \n",
       "2  1478812  airBaltic uzsāks lidojumus uz diviem jauniem g...      0   \n",
       "3  1479295  Ministrs: Sarunas turpinās ar trīs potenciālaj...      0   \n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0   \n",
       "\n",
       "                                       clean_message data_type  \n",
       "0  Tiek vērtēti trīs potenciālie airBaltic invest...     train  \n",
       "1  Augulis: #airBaltic “potenciālie pircēji ir no...       val  \n",
       "2  airBaltic uzsāks lidojumus uz diviem jauniem g...     train  \n",
       "3  Ministrs: Sarunas turpinās ar trīs potenciālaj...     train  \n",
       "4  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...     train  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63617ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./tweets_base.csv', index=False)\n",
    "\n",
    "# df = pd.read_csv('./../labeledTweets/p_n_n_and_labeled_corpus_train_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5ed40",
   "metadata": {},
   "source": [
    "## Balance training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d94d7777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    823\n",
       "2    550\n",
       "1    348\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.data_type=='train']['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "798cd79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiek vērtēti trīs potenciālie airBaltic invest...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ministrs: Sarunas turpinās ar trīs potenciālaj...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1481060</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz Sočiem un Kaļiņi...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsāks lidojumus uz Sočiem un Kaļiņi...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label  \\\n",
       "0  1478404  Tiek vērtēti trīs potenciālie airBaltic invest...      0   \n",
       "2  1478812  airBaltic uzsāks lidojumus uz diviem jauniem g...      0   \n",
       "3  1479295  Ministrs: Sarunas turpinās ar trīs potenciālaj...      0   \n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0   \n",
       "5  1481060  airBaltic uzsāks lidojumus uz Sočiem un Kaļiņi...      0   \n",
       "\n",
       "                                       clean_message data_type  \n",
       "0  Tiek vērtēti trīs potenciālie airBaltic invest...     train  \n",
       "2  airBaltic uzsāks lidojumus uz diviem jauniem g...     train  \n",
       "3  Ministrs: Sarunas turpinās ar trīs potenciālaj...     train  \n",
       "4  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...     train  \n",
       "5  airBaltic uzsāks lidojumus uz Sočiem un Kaļiņi...     train  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.data_type=='train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b9b2669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    348\n",
       "1    348\n",
       "2    348\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = [df[df.data_type=='train'].clean_message, df[df.data_type=='train'].label]\n",
    "df_train = pd.concat(df_train, axis=1, keys=[\"clean_message\", \"label\"])\n",
    "\n",
    "df_0 = df_train[df_train['label']==0]\n",
    "df_1 = df_train[df_train['label']==1]\n",
    "df_2 = df_train[df_train['label']==2]\n",
    "\n",
    "df_0_downsampled = df_0.sample(df_1.shape[0], random_state=42)\n",
    "df_2_downsampled = df_2.sample(df_1.shape[0], random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_0_downsampled, df_2_downsampled, df_1])\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3dff51a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>@laura_freiberga Sveiki! Lūdzu, atsūti DM savu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Tiešraide jau pēc 30 minūtēm!  @hsrigahockey -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>ES ATVAINOJOS, VAI IR KĀDS IZDEVĪGĀKS PAR LATV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>Tiešraide jau pēc 30 minūtēm!  @hsrigahockey -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>SEB ievieš Latvijā pirmo indeksu plānu pensiju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_message  label\n",
       "1526  @laura_freiberga Sveiki! Lūdzu, atsūti DM savu...      0\n",
       "567   Tiešraide jau pēc 30 minūtēm!  @hsrigahockey -...      0\n",
       "260   ES ATVAINOJOS, VAI IR KĀDS IZDEVĪGĀKS PAR LATV...      0\n",
       "560   Tiešraide jau pēc 30 minūtēm!  @hsrigahockey -...      0\n",
       "777   SEB ievieš Latvijā pirmo indeksu plānu pensiju...      0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36da2411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Šodien norisinās SEB MTB maratons. Finišs @val...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>@digidigidong Sveiks! Tas neattiecas uz mūsu e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>@BriedeBriede @kungavsand_ Starpcitu, Maxima t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>Taisnība ir uzvarējusi! Martins Dukurs kļūst p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>@houdshwick Спасибо за мнение.^el</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_message  label\n",
       "700   Šodien norisinās SEB MTB maratons. Finišs @val...      0\n",
       "1911  @digidigidong Sveiks! Tas neattiecas uz mūsu e...      1\n",
       "505   @BriedeBriede @kungavsand_ Starpcitu, Maxima t...      1\n",
       "1471  Taisnība ir uzvarējusi! Martins Dukurs kļūst p...      1\n",
       "1537                  @houdshwick Спасибо за мнение.^el      0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle rows\n",
    "import sklearn\n",
    "\n",
    "df_train = sklearn.utils.shuffle(df_train, random_state=0)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843c4ae",
   "metadata": {},
   "source": [
    "# Tokenizer \"sentence-transformers/LaBSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43b43559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess=\"https://tfhub.dev/google/LaBSE/2\"\n",
    "tfhub_handle_encoder=\"https://tfhub.dev/google/LaBSE/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d86792b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    \n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, trainable=True, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    \n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='LaBSE_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    \n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d0f03be",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"preprocessing\" (type KerasLayer).\n\nin user code:\n\n    File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 237, in call  *\n        result = smart_cond.smart_cond(training,\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None,), dtype=string)\n        * False\n        * None\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask')}\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids')}\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}\n        * False\n        * None\n      Keyword arguments: {}\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None,), dtype=string)\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-57090d42f90e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_classifier_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-decf15ea6e3c>\u001b[0m in \u001b[0;36mbuild_classifier_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpreprocessing_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfhub_handle_preprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'preprocessing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mencoder_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfhub_handle_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'LaBSE_encoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"preprocessing\" (type KerasLayer).\n\nin user code:\n\n    File \"C:\\Users\\aligo\\anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 237, in call  *\n        result = smart_cond.smart_cond(training,\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None,), dtype=string)\n        * False\n        * None\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask')}\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids')}\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * {'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_type_ids'), 'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_mask'), 'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/input_word_ids')}\n        * False\n        * None\n      Keyword arguments: {}\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None,), dtype=string)\n  • training=None"
     ]
    }
   ],
   "source": [
    "model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0655dfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "\n        'EagerTensor' object has no attribute 'size'.\n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-58927d156866>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'this is such an amazing movie!'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mraw_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 944\u001b[1;33m             \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    945\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \"tolist\", \"data\"}:\n\u001b[0;32m    507\u001b[0m       \u001b[1;31m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m       raise AttributeError(\"\"\"\n\u001b[0m\u001b[0;32m    509\u001b[0m         \u001b[1;34m'{}'\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mno\u001b[0m \u001b[0mattribute\u001b[0m \u001b[1;34m'{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mare\u001b[0m \u001b[0mlooking\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mrelated\u001b[0m \u001b[0mmethods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplease\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: \n        'EagerTensor' object has no attribute 'size'.\n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "\n",
    "raw_result = model(tf.constant(text_test))\n",
    "print(tf.sigmoid(raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6bd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a363c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "504acea3",
   "metadata": {},
   "source": [
    "### Find max length for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6b28671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_lens = []\n",
    "for txt in list(df.clean_message.values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "    \n",
    "sns.displot(token_lens)\n",
    "plt.xlim([0, 200])\n",
    "plt.xlabel('Token count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9e882ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6402cf",
   "metadata": {},
   "source": [
    "### Encode messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb945e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_train[\"clean_message\"].values.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].clean_message.values.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c13eef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044, 304)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "582b69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset_train, './datasetsLowercase/dataset_train.pt')\n",
    "# torch.save(dataset_val, './datasetsLowercase/dataset_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "062b659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = torch.load('./datasetsLowercase/dataset_train.pt')\n",
    "# dataset_val = torch.load('./datasetsLowercase/dataset_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f2bfadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11af821",
   "metadata": {},
   "source": [
    "# Model \"sentence-transformers/LaBSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3a7ffbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ce37ece3444b1f96baa55c55215d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('sentence-transformers/LaBSE', num_labels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c906d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16189453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61d95dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bf40782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure weighted F1\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5fc030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c05ccb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model. Returns average validation loss, predictions, true values\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_val, desc='Validating:', leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b36826",
   "metadata": {},
   "source": [
    "# Evaluate untrained LaBSE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4096d6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da36e52e1594032b3dfa6f671589c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b86ac5b3a81d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpreds_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-e8d0d78b4468>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(dataloader_val)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "print(classification_report(true_vals, preds_flat, zero_division=True))\n",
    "pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "        index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "        columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ef371",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81fe5ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f33f0bdc4a411ca840c8900299c62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.0971832925623113\n",
      "Validation loss: 1.0909705877304077\n",
      "F1 Score (Weighted): 0.4069831275876094\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.72      0.66       145\n",
      "           1       0.26      0.53      0.35        62\n",
      "           2       0.43      0.03      0.06        97\n",
      "\n",
      "    accuracy                           0.46       304\n",
      "   macro avg       0.43      0.43      0.36       304\n",
      "weighted avg       0.48      0.46      0.41       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral        105       38        2\n",
      "       positive        27       33        2\n",
      "       negative        39       55        3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 1.0914946180401426\n",
      "Validation loss: 1.0824063777923585\n",
      "F1 Score (Weighted): 0.5628171668841525\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67       145\n",
      "           1       0.36      0.34      0.35        62\n",
      "           2       0.57      0.53      0.55        97\n",
      "\n",
      "    accuracy                           0.57       304\n",
      "   macro avg       0.52      0.52      0.52       304\n",
      "weighted avg       0.56      0.57      0.56       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral        100       20       25\n",
      "       positive        27       21       14\n",
      "       negative        28       18       51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 1.0828923023108281\n",
      "Validation loss: 1.0774479031562805\n",
      "F1 Score (Weighted): 0.5500664314111977\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.52      0.60       145\n",
      "           1       0.32      0.47      0.38        62\n",
      "           2       0.57      0.61      0.59        97\n",
      "\n",
      "    accuracy                           0.54       304\n",
      "   macro avg       0.53      0.53      0.52       304\n",
      "weighted avg       0.58      0.54      0.55       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         76       39       30\n",
      "       positive        18       29       15\n",
      "       negative        15       23       59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 1.0786733229955037\n",
      "Validation loss: 1.0691942930221559\n",
      "F1 Score (Weighted): 0.5594519137927894\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.57      0.61       145\n",
      "           1       0.35      0.35      0.35        62\n",
      "           2       0.56      0.67      0.61        97\n",
      "\n",
      "    accuracy                           0.56       304\n",
      "   macro avg       0.52      0.53      0.53       304\n",
      "weighted avg       0.57      0.56      0.56       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         83       28       34\n",
      "       positive        22       22       18\n",
      "       negative        20       12       65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 1.0728139660575173\n",
      "Validation loss: 1.068424892425537\n",
      "F1 Score (Weighted): 0.5555282381532837\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.54      0.60       145\n",
      "           1       0.33      0.37      0.35        62\n",
      "           2       0.56      0.68      0.62        97\n",
      "\n",
      "    accuracy                           0.55       304\n",
      "   macro avg       0.52      0.53      0.52       304\n",
      "weighted avg       0.57      0.55      0.56       304\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral         79       33       33\n",
      "       positive        21       23       18\n",
      "       negative        17       14       66\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'modelsBase/finetuned_ELECTRA_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "    \n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    \n",
    "    print('Classification report:')\n",
    "    print(classification_report(true_vals, preds_flat))\n",
    "    print('Confusion matrix:')\n",
    "    print(pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "            index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "            columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271f1a9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1b344990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelsBase/finetuned_ELECTRA_epoch_4.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_vals, preds_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "        index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "        columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3840d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38twttr",
   "language": "python",
   "name": "venv38twttr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
