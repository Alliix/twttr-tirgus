{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d90bd2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a01fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = 'maxima'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6490161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMessage(df):\n",
    "    newLine =\"\\\\n|\\\\r\"\n",
    "    urls = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    numbers = '\\d+((\\.|\\-)\\d+)?'\n",
    "    mentions = '\\B\\@([\\w\\-]+)'\n",
    "    hashtag = '#'\n",
    "    whitespaces = '\\s+'\n",
    "    leadTrailWhitespace = '^\\s+|\\s+?$'\n",
    "\n",
    "    df['clean_message'] = df['message']\n",
    "    df['clean_message'] = df['clean_message'].str.replace(newLine,' ',regex=True)\n",
    "    df['clean_message'] = df['clean_message'].str.replace(urls,' URL ',regex=True)\n",
    "    df['clean_message'] = df['clean_message'].str.replace(mentions,' MENTION ',regex=True)\n",
    "    df['clean_message'] = df['clean_message'].str.replace(numbers,' NMBR ',regex=True)\n",
    "    df['clean_message'] = df['clean_message'].str.replace(hashtag,' ',regex=True)\n",
    "    df['clean_message'] = df['clean_message'].str.replace(whitespaces,' ',regex=True)\n",
    "    df['clean_message'] = df['clean_message'].str.replace(leadTrailWhitespace,'',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fa92aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(brand):\n",
    "    tweets = pd.read_csv('./../peopleTweets/'+brand+'.csv')\n",
    "    tweets['brand'] = brand\n",
    "    for index, row in brandTweets.iterrows():\n",
    "#         if not(np.isnan(row['label'])):\n",
    "#             row['label'] = int(row['label'])\n",
    "    cleanMessage(tweets)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e0d5e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>id</th>\n",
       "      <th>tweetId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>language</th>\n",
       "      <th>inReplyToStatusId</th>\n",
       "      <th>inReplyToUserId</th>\n",
       "      <th>inReplyToScreenName</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>...</th>\n",
       "      <th>placeType</th>\n",
       "      <th>retweetedId</th>\n",
       "      <th>monitoringObjectId1</th>\n",
       "      <th>monitoringObjectId2</th>\n",
       "      <th>queryId</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>label</th>\n",
       "      <th>brand</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VienotibaLV @EM_gov_lv Kad sabruka Maxima atk...</td>\n",
       "      <td>1479501</td>\n",
       "      <td>925656400235237400</td>\n",
       "      <td>2017-11-01T11:30:58</td>\n",
       "      <td>lv</td>\n",
       "      <td>9.253513e+17</td>\n",
       "      <td>163366515.0</td>\n",
       "      <td>VienotibaLV</td>\n",
       "      <td>912753641114697700</td>\n",
       "      <td>Chiulju Pussala</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>684</td>\n",
       "      <td>684</td>\n",
       "      <td>923.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maxima</td>\n",
       "      <td>MENTION MENTION Kad sabruka Maxima atkÄpÄs pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KÄ¼uvis zinÄms, kÄdÄ“Ä¼ LatvijÄ krieviski runÄ un...</td>\n",
       "      <td>1479898</td>\n",
       "      <td>925246907835912200</td>\n",
       "      <td>2017-10-31T08:23:47</td>\n",
       "      <td>lv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75507864</td>\n",
       "      <td>MÄris Luste</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>613</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>maxima</td>\n",
       "      <td>KÄ¼uvis zinÄms, kÄdÄ“Ä¼ LatvijÄ krieviski runÄ un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DaceKalnina Un kÄpÄ“c portÄlÄ virs Å¡Ä« raksta e...</td>\n",
       "      <td>1494034</td>\n",
       "      <td>926042411029205000</td>\n",
       "      <td>2017-11-02T13:04:50</td>\n",
       "      <td>lv</td>\n",
       "      <td>9.256348e+17</td>\n",
       "      <td>26082263.0</td>\n",
       "      <td>DaceKalnina</td>\n",
       "      <td>131121745</td>\n",
       "      <td>JÄnis Goldbergs</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>469.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maxima</td>\n",
       "      <td>MENTION Un kÄpÄ“c portÄlÄ virs Å¡Ä« raksta es vie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ‰ğŸ¸ğŸ˜ğŸ˜ğŸ™ğŸ½ (@ Maxima XX - @maximaveikals in Tukums...</td>\n",
       "      <td>1519965</td>\n",
       "      <td>926808202510983200</td>\n",
       "      <td>2017-11-04T15:47:49</td>\n",
       "      <td>sw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2729790340</td>\n",
       "      <td>Lelde</td>\n",
       "      <td>...</td>\n",
       "      <td>city</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>438.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maxima</td>\n",
       "      <td>ğŸ‰ğŸ¸ğŸ˜ğŸ˜ğŸ™ğŸ½ (@ Maxima XX - MENTION in Tukums, Tukum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@GirtsRungainis SkarbÄkais, ka vecÄku absolÅ«ta...</td>\n",
       "      <td>1521601</td>\n",
       "      <td>927041580455874600</td>\n",
       "      <td>2017-11-05T07:15:11</td>\n",
       "      <td>lv</td>\n",
       "      <td>9.268910e+17</td>\n",
       "      <td>134731218.0</td>\n",
       "      <td>GirtsRungainis</td>\n",
       "      <td>701302008</td>\n",
       "      <td>NK</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269</td>\n",
       "      <td>269</td>\n",
       "      <td>508.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>maxima</td>\n",
       "      <td>MENTION SkarbÄkais, ka vecÄku absolÅ«tais vairu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message       id  \\\n",
       "0  @VienotibaLV @EM_gov_lv Kad sabruka Maxima atk...  1479501   \n",
       "1  KÄ¼uvis zinÄms, kÄdÄ“Ä¼ LatvijÄ krieviski runÄ un...  1479898   \n",
       "2  @DaceKalnina Un kÄpÄ“c portÄlÄ virs Å¡Ä« raksta e...  1494034   \n",
       "3  ğŸ‰ğŸ¸ğŸ˜ğŸ˜ğŸ™ğŸ½ (@ Maxima XX - @maximaveikals in Tukums...  1519965   \n",
       "4  @GirtsRungainis SkarbÄkais, ka vecÄku absolÅ«ta...  1521601   \n",
       "\n",
       "              tweetId            createdAt language  inReplyToStatusId  \\\n",
       "0  925656400235237400  2017-11-01T11:30:58       lv       9.253513e+17   \n",
       "1  925246907835912200  2017-10-31T08:23:47       lv                NaN   \n",
       "2  926042411029205000  2017-11-02T13:04:50       lv       9.256348e+17   \n",
       "3  926808202510983200  2017-11-04T15:47:49       sw                NaN   \n",
       "4  927041580455874600  2017-11-05T07:15:11       lv       9.268910e+17   \n",
       "\n",
       "   inReplyToUserId inReplyToScreenName              userId         userName  \\\n",
       "0      163366515.0         VienotibaLV  912753641114697700  Chiulju Pussala   \n",
       "1              NaN                 NaN            75507864      MÄris Luste   \n",
       "2       26082263.0         DaceKalnina           131121745  JÄnis Goldbergs   \n",
       "3              NaN                 NaN          2729790340            Lelde   \n",
       "4      134731218.0      GirtsRungainis           701302008               NK   \n",
       "\n",
       "   ... placeType  retweetedId monitoringObjectId1 monitoringObjectId2 queryId  \\\n",
       "0  ...       NaN          NaN                 684                 684   923.0   \n",
       "1  ...       NaN          NaN                 613                   3     NaN   \n",
       "2  ...       NaN          NaN                 230                 230   469.0   \n",
       "3  ...      city          NaN                 199                 199   438.0   \n",
       "4  ...       NaN          NaN                 269                 269   508.0   \n",
       "\n",
       "   sentiment  retweetCount  label   brand  \\\n",
       "0        NaN             0    NaN  maxima   \n",
       "1     -0.628             0    2.0  maxima   \n",
       "2        NaN             0    NaN  maxima   \n",
       "3        NaN             0    NaN  maxima   \n",
       "4        NaN             0    2.0  maxima   \n",
       "\n",
       "                                       clean_message  \n",
       "0  MENTION MENTION Kad sabruka Maxima atkÄpÄs pre...  \n",
       "1  KÄ¼uvis zinÄms, kÄdÄ“Ä¼ LatvijÄ krieviski runÄ un...  \n",
       "2  MENTION Un kÄpÄ“c portÄlÄ virs Å¡Ä« raksta es vie...  \n",
       "3  ğŸ‰ğŸ¸ğŸ˜ğŸ˜ğŸ™ğŸ½ (@ Maxima XX - MENTION in Tukums, Tukum...  \n",
       "4  MENTION SkarbÄkais, ka vecÄku absolÅ«tais vairu...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brandTweets = readCsv(brand)\n",
    "brandTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af6a33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDates(df):\n",
    "    dates = []\n",
    "    for index, row in df.iterrows():\n",
    "        date_no_0 = row['createdAt'].replace('+00:00','')\n",
    "        dates.append(datetime.datetime.strptime(date_no_0, '%Y-%m-%dT%H:%M:%S').strftime('%Y,%m,%d'))\n",
    "\n",
    "    df[\"date\"] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19ada437",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertDates(brandTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40525b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3bdf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./lvBERT/lvbert_pytorch/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a960c925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_tokens = [('^vi', 34), ('^ai', 30), ('^el', 22), ('(', 19), ('^il', 19), ('â€“', 17), ('^la', 17), ('^na', 17), ('â‚¬', 15), ('^ma', 15), ('^ri', 13), ('+', 11), ('X', 11), (':(', 8), ('Ä»oti', 8), ('^ju', 8), ('ğŸ‡±ğŸ‡»', 7), ('&amp;', 6), ('ğŸ‘‰', 6), ('ğŸ¤”', 5), ('MAXIMA', 5), ('ğŸ„', 5), ('â‚¬.', 4), ('ğŸ˜‰', 4), ('^nn', 4), ('ğŸ˜…', 3), ('|', 3), ('(vai', 3), ('ğŸ˜Š', 3), ('ibankÄ', 3), ('Å½Ä“l,', 3), ('You', 3), ('Ğ½Ğµ', 3), ('Ğ·Ğ°', 3), ('Ğ²', 3), ('â¤ï¸', 3), ('â–¶ï¸', 3), ('uzsâ€¦', 2), ('YL-CSK.', 2), ('(un', 2), ('navâ€¦', 2), ('ğŸ˜', 2), ('(pat', 2), ('=', 2), ('ğŸ¤—', 2), ('Ä¼oooti', 2), ('(kaut', 2), ('XX', 2), ('â€œValmiera/ORDOâ€', 2), ('ğŸ˜€', 2), ('ğŸ™„', 2), ('~', 2), ('ğŸ‡·ğŸ‡º', 2), ('(@', 2), ('*', 2), ('ğŸ¤·\\u200dâ™€ï¸', 2), ('ğŸ¤¬', 2), ('âœ¨', 2), ('(FKTK)', 2), ('(VDD)', 2), ('(sauksim', 2), ('Ğ•ÑĞ»Ğ¸', 2), ('ğŸ™‚', 2), ('Yes,', 2), ('(+', 2), ('Ğ½Ğ°Ñ', 2), ('Ğ”Ğ¾Ğ±Ñ€Ñ‹Ğ¹', 2), ('Ğ´ĞµĞ½ÑŒ!', 2), ('PÄrbaudÄ«sim.^la', 2), ('iOS', 2), ('â€œMaxima', 2), ('[..]', 2), ('XXX', 2), ('ğŸ“²', 2), ('â€œpotenciÄlie', 1), ('pasaulesâ€', 1), ('(maijÄ),', 1), ('â€œ', 1), ('slÄ«pieâ€', 1), ('*FB', 1), ('konkursu*', 1), ('Paâ€¦', 1), ('liepajatravelâ€¦', 1), ('âœˆï¸IzdevÄ«gas', 1), ('laicÄ«giğŸ‘', 1), ('Qatar', 1), ('QR', 1), ('lidmaÅ¡Ä«nasğŸ˜€ğŸ˜€ğŸ˜€', 1), ('(RÄ«gÄ:', 1), ('HavaÅ›)', 1), ('Å…efiga', 1), ('ğŸ‘ğŸ¤—', 1), ('ğŸ˜¥ğŸ˜¥', 1), ('ğŸ˜µ', 1), ('ğŸ‡«ğŸ‡·un', 1), ('krÄÅ¡Å†umÄâ€¦', 1), ('IONITY', 1), ('Ä»ooti', 1), ('skoÄsâ€¦', 1), ('navağŸ˜–', 1)]\n",
    "most_common_values= [word for word, word_count in unk_tokens]\n",
    "\n",
    "tokenizer.add_tokens(most_common_values, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a33ff4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find max length for tokenizer\n",
    "\n",
    "token_lens = []\n",
    "for txt in list(brandTweets.clean_message.values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "    \n",
    "max_length = max(token_lens)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a25c3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "849f0894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5361"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode messages\n",
    "\n",
    "# encoded_data = tokenizer.batch_encode_plus(\n",
    "#     brandTweets[\"clean_message\"].values, \n",
    "#     add_special_tokens=True, \n",
    "#     return_attention_mask=True, \n",
    "#     padding='max_length',\n",
    "#     truncation=True,\n",
    "#     max_length=max_length, \n",
    "#     return_tensors='pt'\n",
    "# )\n",
    "\n",
    "# input_ids = encoded_data['input_ids']\n",
    "# attention_masks = encoded_data['attention_mask']\n",
    "# labels = torch.tensor(brandTweets.label.values)\n",
    "\n",
    "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58bf45fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./lvBERT/lvbert_pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./lvBERT/lvbert_pytorch/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model \"lvBERT\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('./lvBERT/lvbert_pytorch/',\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e32b52b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32104, 768)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95d27055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "\n",
    "model.load_state_dict(torch.load('./lvBERT/addUNKtokens/modelsUNK/finetuned_lvBERT_epoch_1.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c07030f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    token_lens = []\n",
    "    for txt in list(data[\"clean_message\"].values):\n",
    "        tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "        token_lens.append(len(tokens))\n",
    "    max_length = max(token_lens)\n",
    "\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        data[\"clean_message\"].values, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels = torch.tensor(data.label.values)\n",
    "    \n",
    "    outputs = model(**encoded_data)\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    predictions = np.argmax(logits, axis=1).flatten()\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5cfaed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 12187054080 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-63c8fc793189>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlbls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrandTweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlbls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-5c455cd50226>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(model, data)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoded_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1543\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1545\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1546\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    583\u001b[0m                 )\n\u001b[0;32m    584\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[0;32m    514\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   2436\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2438\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 12187054080 bytes."
     ]
    }
   ],
   "source": [
    "predictions, labels = predict(model, brandTweets)\n",
    "print(predictions)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfbcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b16802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38twttr",
   "language": "python",
   "name": "venv38twttr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
