{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a84c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import (ElectraForSequenceClassification, ElectraTokenizerFast)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da5ece",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47471ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsÄks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label\n",
       "0  1478404  Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...      0\n",
       "1  1478695  Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...      0\n",
       "2  1478812  airBaltic uzsÄks lidojumus uz diviem jauniem g...      0\n",
       "3  1479295  Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...      0\n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../../../labeledTweets/allLabeledTweets.csv')\n",
    "df = df[['id', 'message', 'label']]\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cf93da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    965\n",
       "2    645\n",
       "1    410\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42acbbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1478404</td>\n",
       "      <td>Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1478695</td>\n",
       "      <td>Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...</td>\n",
       "      <td>0</td>\n",
       "      <td>Augulis: airBaltic â€œpotenciÄlie pircÄ“ji ir no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1478812</td>\n",
       "      <td>airBaltic uzsÄks lidojumus uz diviem jauniem g...</td>\n",
       "      <td>0</td>\n",
       "      <td>airBaltic uzsÄks lidojumus uz diviem jauniem g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1479295</td>\n",
       "      <td>Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480097</td>\n",
       "      <td>@krisjaniskarins @Janis_Kazocins @EU2017EE Net...</td>\n",
       "      <td>0</td>\n",
       "      <td>MENTION MENTION MENTION NetrÄpÄ«s kÄdam AirBalt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            message  label  \\\n",
       "0  1478404  Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...      0   \n",
       "1  1478695  Augulis: #airBaltic â€œpotenciÄlie pircÄ“ji ir no...      0   \n",
       "2  1478812  airBaltic uzsÄks lidojumus uz diviem jauniem g...      0   \n",
       "3  1479295  Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...      0   \n",
       "4  1480097  @krisjaniskarins @Janis_Kazocins @EU2017EE Net...      0   \n",
       "\n",
       "                                       clean_message  \n",
       "0  Tiek vÄ“rtÄ“ti trÄ«s potenciÄlie airBaltic invest...  \n",
       "1  Augulis: airBaltic â€œpotenciÄlie pircÄ“ji ir no ...  \n",
       "2  airBaltic uzsÄks lidojumus uz diviem jauniem g...  \n",
       "3  Ministrs: Sarunas turpinÄs ar trÄ«s potenciÄlaj...  \n",
       "4  MENTION MENTION MENTION NetrÄpÄ«s kÄdam AirBalt...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newLine =\"\\\\n|\\\\r\"\n",
    "urls = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "numbers = '\\d+((\\.|\\-)\\d+)?'\n",
    "mentions = '\\B\\@([\\w\\-]+)'\n",
    "hashtag = '#'\n",
    "whitespaces = '\\s+'\n",
    "leadTrailWhitespace = '^\\s+|\\s+?$'\n",
    "\n",
    "df['clean_message'] = df['message']\n",
    "df['clean_message'] = df['clean_message'].str.replace(newLine,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(urls,' URL ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(mentions,' MENTION ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(numbers,' NMBR ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(hashtag,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(whitespaces,' ',regex=True)\n",
    "df['clean_message'] = df['clean_message'].str.replace(leadTrailWhitespace,'',regex=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc0d24",
   "metadata": {},
   "source": [
    "# Train, validate split (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584677e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0 = df[df['label']==0]\n",
    "df_1 = df[df['label']==1]\n",
    "df_2 = df[df['label']==2]\n",
    "\n",
    "trainLabelSize = round(df_1.shape[0]*0.85)\n",
    "trainLabelSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b6fe01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    348\n",
       "1    348\n",
       "2    348\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0 = df_0.sample(trainLabelSize, random_state=42)\n",
    "df_1 = df_1.sample(trainLabelSize, random_state=42)\n",
    "df_2 = df_2.sample(trainLabelSize, random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_0, df_1, df_2])\n",
    "# Shuffle rows\n",
    "df_train = sklearn.utils.shuffle(df_train, random_state=42)\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b2abc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    617\n",
       "2    297\n",
       "1     62\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = df.merge(df_train, on=['id', 'message', 'label', 'clean_message'], how='left', indicator=True)\n",
    "df_val = df_val[df_val['_merge']=='left_only']\n",
    "df_val = df_val[['id', 'message', 'label', 'clean_message']]\n",
    "df_val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843c4ae",
   "metadata": {},
   "source": [
    "# Tokenizer \"google/electra-base-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b43559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizerFast.from_pretrained('google/electra-base-discriminator', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed230b",
   "metadata": {},
   "source": [
    "## Find popular Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb53fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ğŸ˜‚', 12), ('ğŸ¤”', 9), ('ğŸ˜', 9), ('ğŸ‘', 7), ('ğŸ‘‰', 6), ('ğŸ™„', 6), ('ğŸ˜€', 5), ('ğŸ˜‰', 5), ('ğŸ˜', 5), ('ğŸ˜Š', 4), ('ğŸ¤—', 3), ('ğŸ˜…', 3), ('ğŸ‘', 3), ('â¤ï¸', 3), ('ğŸ˜¥', 2), ('ğŸ˜', 2), ('ğŸ˜†', 2), ('ğŸ¤˜', 2), ('ğŸ¤¬', 2), ('ğŸ™‚', 2), ('ğŸ˜µ', 1), ('ğŸ˜–', 1), ('ğŸ¤¥', 1), ('â˜ï¸', 1), ('ğŸ™', 1), ('ğŸ¤£', 1), ('ğŸ˜³', 1), ('ğŸ™Œ', 1), ('ğŸ‘Œ', 1), ('ğŸ¤“', 1), ('ğŸ˜Œ', 1), ('ğŸ¤¢', 1), ('ğŸ‘‡', 1), ('ğŸ˜œ', 1)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "emoji_str = \"ğŸ˜€ ğŸ˜ƒ ğŸ˜„ ğŸ˜ ğŸ˜† ğŸ˜… ğŸ˜‚ ğŸ¤£ ğŸ˜Š ğŸ˜‡ ğŸ™‚ ğŸ™ƒ ğŸ˜‰ ğŸ˜Œ ğŸ˜ ğŸ¥° ğŸ˜˜ ğŸ˜— ğŸ˜™ ğŸ˜š ğŸ˜‹ ğŸ˜› ğŸ˜ ğŸ˜œ ğŸ¤ª ğŸ¤¨ ğŸ§ ğŸ¤“ ğŸ˜ ğŸ¤© ğŸ¥³ ğŸ˜ \n",
    "ğŸ˜’ ğŸ˜ ğŸ˜” ğŸ˜Ÿ ğŸ˜• ğŸ™ â˜¹ï¸ ğŸ˜£ ğŸ˜– ğŸ˜« ğŸ˜© ğŸ¥º ğŸ˜¢ ğŸ˜­ ğŸ˜¤ ğŸ˜  ğŸ˜¡ ğŸ¤¬ ğŸ¤¯ ğŸ˜³ ğŸ¥µ ğŸ¥¶ ğŸ˜± ğŸ˜¨ ğŸ˜° ğŸ˜¥ ğŸ˜“ ğŸ¤— ğŸ¤” ğŸ¤­ ğŸ¤« ğŸ¤¥ ğŸ˜¶ ğŸ˜ ğŸ˜‘ \n",
    "ğŸ˜¬ ğŸ™„ ğŸ˜¯ ğŸ˜¦ ğŸ˜§ ğŸ˜® ğŸ˜² ğŸ¥± ğŸ˜´ ğŸ¤¤ ğŸ˜ª ğŸ˜µ ğŸ¤ ğŸ¥´ ğŸ¤¢ ğŸ¤® ğŸ¤§ ğŸ˜· ğŸ¤’ ğŸ¤• ğŸ˜º ğŸ˜¸ ğŸ˜¹ ğŸ˜» ğŸ˜¼ ğŸ˜½ ğŸ™€ ğŸ˜¿ ğŸ˜¾ ğŸ’© ğŸ‘‹ ğŸ¤š ğŸ– âœ‹ ğŸ–– \n",
    "ğŸ‘Œ ğŸ¤ âœŒï¸ ğŸ¤ ğŸ¤Ÿ ğŸ¤˜ ğŸ¤™ ğŸ‘ˆ ğŸ‘‰ ğŸ‘† ğŸ–• ğŸ‘‡ â˜ï¸ ğŸ‘ ğŸ‘ âœŠ ğŸ‘Š ğŸ¤› ğŸ¤œ ğŸ‘ ğŸ™Œ ğŸ‘ ğŸ¤² ğŸ¤ ğŸ™'ğŸ‘‹ğŸ» ğŸ¤šğŸ» ğŸ–ğŸ» âœ‹ğŸ» ğŸ––ğŸ» ğŸ‘ŒğŸ» ğŸ¤ğŸ» âœŒğŸ» ğŸ¤ğŸ» ğŸ¤ŸğŸ» ğŸ¤˜ğŸ» ğŸ¤™ğŸ» ğŸ‘ˆğŸ» ğŸ‘‰ğŸ» \n",
    "ğŸ‘†ğŸ» ğŸ–•ğŸ» ğŸ‘‡ğŸ» â˜ğŸ» ğŸ‘ğŸ» ğŸ‘ğŸ» âœŠğŸ» ğŸ‘ŠğŸ» ğŸ¤›ğŸ» ğŸ¤œğŸ» ğŸ‘ğŸ» ğŸ™ŒğŸ» ğŸ‘ğŸ» ğŸ¤²ğŸ» ğŸ™ğŸ» ğŸ‘‹ğŸ¼ ğŸ¤šğŸ¼ ğŸ–ğŸ¼ âœ‹ğŸ¼ ğŸ––ğŸ¼ ğŸ‘ŒğŸ¼ ğŸ¤ğŸ¼ âœŒğŸ¼ ğŸ¤ğŸ¼ ğŸ¤ŸğŸ¼ ğŸ¤˜ğŸ¼ ğŸ¤™ğŸ¼ ğŸ‘ˆğŸ¼ ğŸ‘‰ğŸ¼ ğŸ‘†ğŸ¼ ğŸ–•ğŸ¼ ğŸ‘‡ğŸ¼ â˜ğŸ¼ ğŸ‘ğŸ¼ ğŸ‘ğŸ¼ âœŠğŸ¼ ğŸ‘ŠğŸ¼ ğŸ¤›ğŸ¼ ğŸ¤œğŸ¼ \n",
    "ğŸ‘ğŸ¼ ğŸ™ŒğŸ¼ ğŸ‘ğŸ¼ ğŸ¤²ğŸ¼ ğŸ™ğŸ¼ ğŸ‘‹ğŸ½ ğŸ¤šğŸ½ ğŸ–ğŸ½ âœ‹ğŸ½ ğŸ––ğŸ½ ğŸ‘ŒğŸ½ ğŸ¤ğŸ½ âœŒğŸ½ ğŸ¤ğŸ½ ğŸ¤ŸğŸ½ ğŸ¤˜ğŸ½ ğŸ¤™ğŸ½ ğŸ‘ˆğŸ½ ğŸ‘‰ğŸ½ ğŸ‘†ğŸ½ ğŸ–•ğŸ½ ğŸ‘‡ğŸ½ â˜ğŸ½ ğŸ‘ğŸ½ ğŸ‘ğŸ½ âœŠğŸ½ ğŸ‘ŠğŸ½ ğŸ¤›ğŸ½ ğŸ¤œğŸ½ ğŸ‘ğŸ½ ğŸ™ŒğŸ½ ğŸ‘ğŸ½ ğŸ¤²ğŸ½ ğŸ™ğŸ‘‹ğŸ¾ ğŸ¤šğŸ¾ ğŸ–ğŸ¾ âœ‹ğŸ¾ ğŸ––ğŸ¾ \n",
    "ğŸ‘ŒğŸ¾ ğŸ¤ğŸ¾ âœŒğŸ¾ ğŸ¤ğŸ¾ ğŸ¤ŸğŸ¾ ğŸ¤˜ğŸ¾ ğŸ¤™ğŸ¾ ğŸ‘ˆğŸ¾ ğŸ‘‰ğŸ¾ ğŸ‘†ğŸ¾ ğŸ–•ğŸ¾ ğŸ‘‡ğŸ¾ â˜ğŸ¾ ğŸ‘ğŸ¾ ğŸ‘ğŸ¾ âœŠğŸ¾ ğŸ‘ŠğŸ¾ ğŸ¤›ğŸ¾ ğŸ¤œğŸ¾ ğŸ‘ğŸ¾ ğŸ™ŒğŸ¾ ğŸ‘ğŸ¾ ğŸ¤²ğŸ¾ ğŸ™ ğŸ‘‹ğŸ¿ ğŸ¤šğŸ¿ ğŸ–ğŸ¿ âœ‹ğŸ¿ ğŸ––ğŸ¿ ğŸ‘ŒğŸ¿ ğŸ¤ğŸ¿ âœŒğŸ¿ ğŸ¤ğŸ¿ ğŸ¤ŸğŸ¿ ğŸ¤˜ğŸ¿ ğŸ¤™ğŸ¿ ğŸ‘ˆğŸ¿ ğŸ‘‰ğŸ¿ ğŸ‘†ğŸ¿ \n",
    "ğŸ–•ğŸ¿ ğŸ‘‡ğŸ¿ â˜ğŸ¿ ğŸ‘ğŸ¿ ğŸ‘ğŸ¿ âœŠğŸ¿ ğŸ‘ŠğŸ¿ ğŸ¤›ğŸ¿ ğŸ¤œğŸ¿ ğŸ‘ğŸ¿ ğŸ™ŒğŸ¿ ğŸ‘ğŸ¿ ğŸ¤²ğŸ¿ ğŸ™ğŸ¿ â¤ï¸ ğŸ§¡ ğŸ’› ğŸ’š ğŸ’™ ğŸ’œ ğŸ–¤ ğŸ¤ ğŸ¤ ğŸ’” â£ï¸ ğŸ’• ğŸ’ ğŸ’“ ğŸ’— ğŸ’– ğŸ’˜ ğŸ’ ğŸ’Ÿ ğŸ’‘ğŸ» ğŸ’‘ğŸ¼ ğŸ’‘ğŸ½ \n",
    "ğŸ’‘ğŸ¾ ğŸ’‘ğŸ¿ ğŸ’ğŸ» ğŸ’ğŸ¼ ğŸ’ğŸ½ ğŸ’ğŸ¾ ğŸ’ğŸ¿ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ½ \n",
    "ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¼ \n",
    "ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ¿ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ» \n",
    "ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ§‘ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ \n",
    "ğŸ‘¨ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ \n",
    "ğŸ‘©ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¿ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ» ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¼ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ½ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¾ ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘©ğŸ¿ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ \n",
    "ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ§‘ğŸ½â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ \n",
    "ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¾â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¿ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ» ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¼ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ½ ğŸ§‘ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ ğŸ‘­ ğŸ§‘â€ğŸ¤â€ğŸ§‘ ğŸ‘¬ ğŸ‘« ğŸ‘©â€â¤ï¸â€ğŸ‘© ğŸ’‘ ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘© ğŸ’ \n",
    "ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ ğŸ‘ª ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ ğŸ‘§â€ğŸ‘§ ğŸ’‹\"\n",
    "\n",
    "\n",
    "emoji_list = emoji_str.split(' ')\n",
    "emoji_regex = '|'.join(emoji_list)\n",
    "\n",
    "all_emoji = []\n",
    "for message in df.clean_message:\n",
    "    foundEmoji = re.findall(emoji_regex, message)\n",
    "    for emoji in foundEmoji:\n",
    "        all_emoji.append(emoji)\n",
    "        \n",
    "import collections\n",
    "\n",
    "counter=collections.Counter(all_emoji)\n",
    "print(counter.most_common(100))\n",
    "\n",
    "most_common_values= [word for word, word_count in counter.most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173ced02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(most_common_values, special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504acea3",
   "metadata": {},
   "source": [
    "### Find max length for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b28671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lens = []\n",
    "for txt in list(df.clean_message.values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "    \n",
    "max_length = max(token_lens)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6402cf",
   "metadata": {},
   "source": [
    "### Encode messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb945e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044, 976)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_train[\"clean_message\"].values.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df_val[\"clean_message\"].values.tolist(), \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df_val.label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11af821",
   "metadata": {},
   "source": [
    "# Model \"google/electra-base-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a7ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bf87bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30556, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c906d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16189453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61d95dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf40782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure weighted F1\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5fc030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c05ccb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model. Returns average validation loss, predictions, true values\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_val, desc='Validating:', leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b36826",
   "metadata": {},
   "source": [
    "# Evaluate untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4096d6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.27      0.35       617\n",
      "           1       0.07      0.32      0.12        62\n",
      "           2       0.31      0.39      0.35       297\n",
      "\n",
      "    accuracy                           0.31       976\n",
      "   macro avg       0.30      0.33      0.27       976\n",
      "weighted avg       0.42      0.31      0.33       976\n",
      "\n",
      "0.3342284002069008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>neutral</th>\n",
       "      <td>165</td>\n",
       "      <td>217</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>140</td>\n",
       "      <td>41</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predicted                  \n",
       "                  neutral positive negative\n",
       "actual neutral        165      217      235\n",
       "       positive        21       20       21\n",
       "       negative       140       41      116"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "print(classification_report(true_vals, preds_flat))\n",
    "print(f1_score_func(predictions, true_vals))\n",
    "pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "        index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "        columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ef371",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81fe5ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66803d63830b4d35b870b5747b1df00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.091834104422367\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.0592548328061258\n",
      "F1 Score (Weighted): 0.6225254972235129\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72       617\n",
      "           1       0.09      0.13      0.10        62\n",
      "           2       0.51      0.56      0.53       297\n",
      "\n",
      "    accuracy                           0.61       976\n",
      "   macro avg       0.45      0.46      0.45       976\n",
      "weighted avg       0.64      0.61      0.62       976\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral        420       56      141\n",
      "       positive        32        8       22\n",
      "       negative       101       29      167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 1.0555660833011975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.0177865278336309\n",
      "F1 Score (Weighted): 0.6213128657026317\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.62      0.70       617\n",
      "           1       0.13      0.16      0.14        62\n",
      "           2       0.48      0.69      0.57       297\n",
      "\n",
      "    accuracy                           0.61       976\n",
      "   macro avg       0.47      0.49      0.47       976\n",
      "weighted avg       0.66      0.61      0.62       976\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral        380       46      191\n",
      "       positive        23       10       29\n",
      "       negative        72       21      204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 1.0235736388148684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.0054094983685402\n",
      "F1 Score (Weighted): 0.6059286716570691\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.57      0.67       617\n",
      "           1       0.10      0.18      0.13        62\n",
      "           2       0.48      0.70      0.57       297\n",
      "\n",
      "    accuracy                           0.59       976\n",
      "   macro avg       0.47      0.48      0.46       976\n",
      "weighted avg       0.67      0.59      0.61       976\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral        351       72      194\n",
      "       positive        22       11       29\n",
      "       negative        58       30      209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.9969850019975142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.9893573253385483\n",
      "F1 Score (Weighted): 0.6175000582498387\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68       617\n",
      "           1       0.13      0.29      0.18        62\n",
      "           2       0.51      0.69      0.59       297\n",
      "\n",
      "    accuracy                           0.59       976\n",
      "   macro avg       0.49      0.52      0.48       976\n",
      "weighted avg       0.68      0.59      0.62       976\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral        354       86      177\n",
      "       positive        21       18       23\n",
      "       negative        54       37      206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.9758911114750486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating::   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.9921267436396691\n",
      "F1 Score (Weighted): 0.6094764057989027\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.55      0.66       617\n",
      "           1       0.13      0.29      0.18        62\n",
      "           2       0.50      0.72      0.59       297\n",
      "\n",
      "    accuracy                           0.59       976\n",
      "   macro avg       0.49      0.52      0.48       976\n",
      "weighted avg       0.69      0.59      0.61       976\n",
      "\n",
      "Confusion matrix:\n",
      "                predicted                  \n",
      "                  neutral positive negative\n",
      "actual neutral        339       89      189\n",
      "       positive        19       18       25\n",
      "       negative        49       34      214\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "    torch.save(model.state_dict(), f'modelsEmoji/finetuned_ELECTRAbase_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "    \n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    \n",
    "    print('Classification report:')\n",
    "    print(classification_report(true_vals, preds_flat))\n",
    "    print('Confusion matrix:')\n",
    "    print(pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "            index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "            columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271f1a9",
   "metadata": {},
   "source": [
    "# Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1b344990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelsBase/finetuned_ELECTRAbase_epoch_X.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score_func(predictions, true_vals))\n",
    "print(classification_report(true_vals, preds_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(true_vals, preds_flat),\n",
    "        index = [['actual', 'actual', 'actual'], ['neutral', 'positive', 'negative']],\n",
    "        columns = [['predicted', 'predicted', 'predicted'], ['neutral', 'positive', 'negative']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38twttr",
   "language": "python",
   "name": "venv38twttr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
